---
title: "Statistics with R - session 2"
author: Géraldine Derroire
institute: Cirad - UnB
date: last-modified
format: 
  revealjs:
    theme: solarized
    output-location: fragment 
    slide-number: true
    preview-links: true
    chalkboard: true
    link-external-icon: true
    link-external-newwindow: true
    incremental: true
execute:
  echo: true   
  warning: true
  message: true 
  cache: true
editor: 
  markdown: 
    wrap: sentence
---

#

Let's load the packages that we will use today

```{r}
#| message: false
library(tidyverse)
library(GGally)
```

::: notes
GGally to see pair-correlation
:::

#

Last session, we saw how to test for differences between variables.

Today we are going to look at **relationships between variables**.


# Covariance and correlation

## Covariance {.scrollable}

We have seen that the variance of a variable *x* is the mean of squared deviations from the mean:

$$\sigma^2_x = \frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n}$$

::: fragment

[The covariance between two quantitative variables *x* and *y* is the mean of the product of the deviations of *x* and *y* from their respective mean:]{style="font-size: 30px"}

$$cov_{xy} = \frac{\sum_{i=1}^n (x_i-\bar{x}) \times (y_i-\bar{y})}{n}$$

:::

## Covariance

We use the function [cov]{style="color:indianred;"} to get the covariance between two variables.

Let's test this with the iris data set, which you graphically explore in a last exercise. We focus on the species *versicolor*:

```{r}
data(iris)
iris <- as_tibble(iris)
iris_vers <- iris %>% filter(Species == "versicolor")
cov(iris_vers$Petal.Length, iris_vers$Petal.Width)
```
[[⚠️ The unit of the covariance is the product of the unit of x and of y...]{style="font-size: 30px"}]{.fragment}


## Correlation of Pearson

[To help with the interpretation of the covariance, we standardise the covariance by the product of the standard deviations.]{style="font-size: 30px"}

[This is the **coefficient of correlation of Pearson**, which is between -1 and 1:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

* [a coefficient = -1 (respectively 1) means that the variables are perfectly negatively (respectively positively) correlated]{style="font-size: 30px"}

* [a coefficient = 0 shows an absence of correlation]{style="font-size: 30px"}
:::

::: {.column width="50%"}

[[We use the function [cor]{style="color:indianred;"} to get the correlation between two variables (the default method is Pearson correlation).]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
cor(iris_vers$Petal.Length, 
    iris_vers$Petal.Width)
```
:::

[[*There is a strong positive correlation.*]{style="font-size: 30px"}]{.fragment}

:::
:::::


::: notes
So here strong positive correlation
:::

## Correlation of Spearman

[Pearson correlation makes the hypothesis that the relationship is linear.]{style="font-size: 30px"}

[If this is not the case, we can use the *rank correlation of Spearman*, by specifying the method:]{style="font-size: 30px"}

```{r}
cor(iris_vers$Petal.Length, iris_vers$Petal.Width,
    method = "spearman")
```
::: notes
here, not much difference
:::

## Test of correlation

[To test if the correlation is significant, we do a test of correlation with the function [cor.test]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
cor.test(iris_vers$Petal.Length, iris_vers$Petal.Width,
         method = "pearson")
```

[[*The correlation is significant.*]{style="font-size: 30px"}]{.fragment}

## Visualise correlations 

[Pearson correlations between multiple pairs of variables can be visualised using the function [ggpairs]{style="color:indianred;"} of the package [GGally]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
#| out.width: 80%
ggpairs(iris_vers, columns = c("Sepal.Length", "Sepal.Width",
                               "Petal.Length", "Petal.Width"))
# this is equivalent to 
 # ggpairs(iris_vers, columns = 1:4)
```

::: notes
we get the correlation coef and its significance

more complicated if we want other type of correlation than Pearson
:::


# Simple linear regression

## Nouragues tree data {.scrollable}

[We will work on a modified version of Nouragues tree data from the package BIOMASS (available     [here](https://geraldinederroire.github.io/Course_R_Forest_Sciences/6_basic_statistics/data/data_statistics_2.RData){preview-link="false"})]{style="font-size: 30px"}
[These data have been modified to exclude the non fully determined trees, and add the family.]{style="font-size: 30px"}

[*Heights and diameters of trees in two 1-ha plots from the Nouragues forest (French Guiana)*]{style="font-size: 30px"}


```{r}
load("data/data_statistics_2.RData")
dt_nou
```


<!-- could be plant growth rate as a function of soil humidity:
https://pmarchand1.github.io/ECL7102/notes_cours/6-Regression_lineaire.html

or maybe the data from the Nouragues
=> modelling heigth ~ D
=> then talking about log transfor
=> selection of the best model
! voir si je peux aussi m'en servir pour tester avec Category? Famille (à importer avec Biomass) ou plot?
=> tester puis rajouter interaction entre D et famille?
=> et effet aléatoire famille?
--> 

## Relationship between D and H

[Let's explore the relationship between tree diameter and height graphically:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out.width: 80%
g <- ggplot(dt_nou, aes(x = D, y = H)) + 
  geom_point() 
g + geom_smooth(method = "lm", se = FALSE)
```


## Structure of a linear regression

A linear regression is a model of the mathematical relationship between:

* A **response variable**, also called *dependent variable*

* One or several **explanatory variables**, also called *predictors* or *independent variables*

[[In our case, we want to model the relationship between *D* (the explanatory variable) and *H* (the response variable).]{style="font-size: 30px"}]{.fragment}

[[As our model has only one explanatory variable, it is called *simple* linear regression.]{style="font-size: 30px"}]{.fragment}



## Uses of regressions

* Study the relationship between several variables (without assuming causality)

* Study the effect of numerical or categorical variables on another variables 

* Use these relationships to predict the values of the modelled variable for new observations.

[[In our case, we want to model a tree dimension by another tree dimension. This is an *allometric model*. Allometric models are often used to predict a variable difficult to measure from a variable more broadly available]{style="font-size: 30px"}]{.fragment}


## Theory 

$$y = \alpha + \beta \times x + \epsilon$$

::::: columns
::: {.column width="50%"}

* [x is the response variable]{style="font-size: 30px"}

* [y is the explanatory variable]{style="font-size: 30px"}

* [$\alpha$ is the **intercept** (the mean of Y when X = 0)]{style="font-size: 30px"}

* [$\beta$ is the **slope** of the relationship between X and Y]{style="font-size: 30px"}

* [$\epsilon$ is the **residual** of the model. $\epsilon \sim \mathcal N(0, \sigma^2)$]{style="font-size: 30px"}

* [$\alpha$, $\beta$ and $\epsilon$ are the **parameters** of the model]{style="font-size: 30px"}
:::

::: {.column width="50%"}

::: fragment
[The model can also be written:]{style="font-size: 30px"}

$$y \sim \mathcal N(\alpha + \beta \times x,  \sigma^2)$$
:::
:::
:::::

::: notes
who want to write the equation of the model we want to fit on the blackboard?
:::


## Theory 

$$y \sim \mathcal N(\alpha + \beta \times x,  \sigma^2)$$

[The model predicts a probability density of Y  for any value of X normally distributed around the regression line.]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

![](lm.png){width="200%"}
[Source: [E. Marcon](https://ericmarcon.github.io/Cours-R-Geeft/Regression_lineaire.html#6){preview-link="false"}]{style="font-size: 30px"}
:::


::: {.column width="50%"}
* [The mean value of Y for any value of X is the black line : $\mu = \alpha + \beta \times x$]{style="font-size: 30px"}

* [The predicted probability density of Y are represented by the red dots: they are normally distributed around the mean with a variance $\sigma^2$]{style="font-size: 30px"}


:::
:::::

::: notes
read the fig as in 3D
:::


## Fitting a simple linear regression

[We use the function [lm]{style="color:indianred;"} to fit a linear regression:]{style="font-size: 30px"}

```{r}
mod_dh_1 <- lm(H ~ D, # formula
               data = dt_nou)
```

Syntax of the formula:

* response variable ~ explanatory variable(s)

* the intercept is implicit so *H ~ D* is equivalent to *H ~ 1 + D*

[[If we don't want an intercept, we have to write *H ~ 0 + D*. The regression will go through the origin (0,0).]{style="font-size: 30px"}]{.fragment}


## Interpreting the output {.smaller}


::::: columns
::: {.column width="50%"}

[$$y = \alpha + \beta \times x + \epsilon$$]{style="font-size: 30px"}

```{r}
summary(mod_dh_1)
```
:::

::: {.column width="50%"}
* [*Coefficients* give the estimates of **intercept** ($\alpha$) and the **slope** ($\beta$).]{style="font-size: 30px"}

  * *intercept*: the value of H for D=0 is 0.325
  
  * *slope*: for every increase of 1 cm DBH, increase of 0.32 m in height
  
  * [A t-test is performed to see if there are significant (significantly different from 0).]{style="font-size: 30px"}

* [*Residual standard error* is the estimate of the **standard deviation** of the residuals ($\sigma$).]{style="font-size: 30px"}

:::
::::::

::: notes
BUT don't use that to predict outside of the range of observed data
:::

## Interpreting the output {.smaller}


::::: columns
::: {.column width="50%"}

[$$y = \alpha + \beta \times x + \epsilon$$]{style="font-size: 30px"}

```{r}
summary(mod_dh_1)
```
:::

::: {.column width="50%"}
* [*R^2^ *(discussed after)]{style="font-size: 30px"}

* [The *F-statistic* test if the model is significantly different from a null model, *i.e.* if it explains a **significant amount of the variation** on the response variable.]{style="font-size: 30px"}

:::
::::::

::: notes
Null model : a model without explanatory variable
F-test for only one single explanatory variable is the same as the test for the explanatory variable
:::

## Coefficient of determination 

The **R^2^** is the coefficient of determination: it represents the **fraction of variance explained** by the model. The closer the *R^2^* is to 1, the greater the explanatory power of the model.

[The adjusted *R^2^* penalise the *R^2^* by the number of explanatory variables (not relevant when there is just one explanatory variable).]{.fragment}

## *R^2^* and *p-value*

* The **p-value** indicates **significance of the effect** of the explanatory variable: this effect can be small yet highly significant => a small p-value is particularly important for hypothesis testing.


* The **R^2^** indicates the **goodness of fit** (how well the model fits the observed data) => a high *R^2^* is particularly important for prediction.


## Confidence interval

The **confidence interval** is the interval of confidence of the mean predicted value of y (so of $\mu$).

[[To calculate it, we can use the function [predict]{style="color:indianred;"}, with the argument *interval = "confidence"*, on new data that we create for the prediction:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
new <- tibble(D = seq(from = 10, to = 160, by= 10)) # create new data
new_conf <- predict(mod_dh_1, newdata = new, 
                    interval = "confidence") # calculate the confidence interval
new_conf <- bind_cols(new, new_conf) # combine the two
slice_head(new_conf, n=3) # visualise them to check
```
:::

::: notes
why do we create new data? to do less predictions than if we were using the observed D 

no need to take too many points because we have at line
:::

## Confidence interval

[We can then plot the confidence interval:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out-width: 70%
gpred <- ggplot(new_conf, aes(x= D)) +
  geom_line(aes(y = fit), color = "blue") + # prediction mean
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, color = "darkgrey") + # CI
  geom_point(data = dt_nou, aes(x = D, y = H), alpha = 0.3) + # observations
  theme_bw()
gpred
```




## Prediction interval

TELL WHAT ARE THE PREDICTION + COMMENT THE CODE

```{r}
new_pred <- predict(mod_dh_1, newdata = new,
                    interval = "prediction")
new_pred <- bind_cols(new, new_pred)
slice_head(new_pred)
```

```{r}
#| warning: false
gpred <- gpred +
  geom_ribbon(data = new_pred, aes(ymin = lwr, ymax = upr), 
              alpha = 0.3, color = "lightgrey") +
  theme_bw()
gpred
```

## Model validation

We need to check the model assumptions.

* normality of the residuals

* independence of the residuals

* constant variance of the residuals (homoscedasticity)

* linearity of the relationship between response and explanatory variables

## Homoscedasticity and independence of the residuals

::::: columns
::: {.column width="50%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 1)
```

:::


::: {.column width="50%"}

::: fragment
[⚠️ Here the residuals depends on the fitted values => this suggests that the relationship is not linear...]{style="font-size: 30px"}

[⚠️ The variance is not constant.]{style="font-size: 30px"}
:::

:::
:::::


## Normality of the residuals

::::: columns
::: {.column width="50%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 2)
```

:::


::: {.column width="50%"}
[[✅ Here the residuals are reasonably normality distributed.]{style="font-size: 30px"}]{.fragment}
:::
:::::


## Leverage effect

::::: columns
::: {.column width="40%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 4)
```

:::


::: {.column width="60%"}

::: fragment
[⚠️️ Some observations are highly influential.]{style="font-size: 30px"}

:::

:::
:::::


## Variable transformation

[Our model has problem of linearity and high leverage of some observations.]{style="font-size: 30px"}

::: fragment
[💡 A transformation of variables may help, let's try to take the log of D and H:]{style="font-size: 30px"}

$$log(H) = \alpha + \beta \times log(D)$$
:::


::: fragment

[which is equivalent to
$y = \alpha'x^\beta$ with $\alpha' = e^{\alpha}$
because]{style="font-size: 30px"}

[$log\big (\alpha'x^\beta \big) = log(\alpha') + \beta \times log(D)$]{style="font-size: 30px"}
:::

[[*This is a classical allometric model: it predicts that the height of the tree will increase less rapidly for trees with bigger DBH, which is consistent with what we expect.*]{style="font-size: 30px"}]{.fragment}


## Fitting the log-log model {.smaller}

```{r}
mod_dh_2 <- lm(log(H) ~ log(D), data = dt_nou)
summary(mod_dh_2)
```

## Model validation {.smaller}

::::: columns
::: {.column width="33%"}
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 1)
```

[[😊 We have improved the independance of residuals and homoscedasticity.]{style="font-size: 30px"}]{.fragment}
:::


::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 2)
```
:::
[[😊 The residuals are still normally distributed.]{style="font-size: 30px"}]{.fragment}
:::

::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 4)
```
:::

[[😊 We have considerably reduced the leverage effect.]{style="font-size: 30px"}]{.fragment}

:::
:::::



## comparaison de modèle

Dire qu'on préfère le log-log :

less pb with model validation, slighly better r2 and more ecological meaning

entre le lineaire et le log-log !!!!! not with the AIC so more in the next section on multiple linear regression




## predire avec des nouvelles valeurs

quand transfo de données: 

```{r}
# library(ggeffects)
# library(ggplot2)
# 
# # Fit your model
# model <- lm(log(H) ~ log(D), data = your_data)
# 
# # Get predictions back on original scale
# preds <- ggpredict(model, terms = "D", type = "re")  # "re" = response scale (exp back-transform)
# 
# ggplot(preds, aes(x = x, y = predicted)) +
#   geom_line(color = "blue", size = 1) +
#   geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "blue", alpha = 0.2) +
#   geom_point(data = dt_nou, aes(x = D, y = H), alpha = 0.6) +
#   labs(x = "D", y = "H", title = "Predicted H vs D (log-log model, back-transformed)") +
#   theme_minimal()
```




## presentation des résultats

cf lamaragnge: https://larmarange.github.io/guide-R/analyses/regression-lineaire.html

gtsummary 


## what if the predictor is categorical

* show the equation et do it quickly

compare ANOVA and lm (Marchand)

à faire avec quelques familles de Nouragues?

ne pas faire tout le diagnostic...



# Multiple linear regression - One quanti and one quali

maybe go directly to several quanti... so I can talk about collinearity and selection

follow marchand et Eric 

voir si je parle de emmean ou plutôt de ggstats::ggcoef_model() dans Lamaranche

## Data

je pourrais continuer avec les H/diam mais qu'un seul prédicteur continue...

# Multiple linear regression - Several quanti

follow marchand et Eric 

## Data
????? could use the msleep de ggplot comme Marchand?
ou iris mais !!!! pb de collinearity...

## talk about collinearity of predictor

cf Eric 35 and VIF (cf marchand cours 8)

## standardisation des variables

cf Eric 37

## interaction between variable

Eric 36


## model selection
Eric 57: AIC

et broom https://larmarange.github.io/guide-R/analyses/selection-modele-pas-a-pas.html#afficher-les-indicateurs-de-performance



# TO GO FURTHER (not in depth here...)

# What if the observation are not independant ? mixed-effect models

Nouragues per plot?


# What if the data are not normal ? GLM

## Logistic regression

karnataka pres/abs of a family per plot? mais pas de prédicteur?
or BCI?

## Poisson

karnataka count of a family per plot? mais pas de prédicteur?
or BCI?






# Acknowledgments {.smaller}

::: {.nonincremental}

* Marchand P. *Analyses et modélisation des données écologiques* [in French](https://pmarchand1.github.io/ECL7102/){preview-link="false"}

* Marcon E. *cours-R-Geeft* [in French](https://ericmarcon.github.io/Cours-R-Geeft/){preview-link="false"}

<!--
* *Introduction à l’analyse d’enquêtes avec R et RStudio* - Julien Barnier, Julien Biaudet, François Briatte, Milan Bouchet-Valat, Ewen Gallic, Frédérique Giraud, Joël Gombin, Mayeul Kauffmann, Christophe Lalanne, Joseph Larmarange, Nicolas Robette [in French](https://larmarange.github.io/analyse-R/){preview-link="false"} 

* Barnier J. *Introduction à R et au tidyverse* [in French](https://juba.github.io/tidyverse/){preview-link="false"}

* Rech R. *One-Way ANOVA and Boxplot in R* [here](https://statdoe.com/one-way-anova-and-box-plot-in-r/){preview-link="false"}
-->

:::

# Ressources

::: {.nonincremental}

<!--

* Dytham C. *Choosing and using statistics - a biologist's guide*, 3rd Edition. Available [here](http://ngc.digitallibrary.co.in/bitstream/123456789/2272/1/Choosing%20and%20using%20statistics_A%20Biologist%27s%20Guide%20%283rd%20edition%29%40Dytham.pdf){preview-link="false"}

* Rech R. *One-Way ANOVA and Boxplot in R* [here](https://statdoe.com/one-way-anova-and-box-plot-in-r/){preview-link="false"}

* datacamp tutorials on [chi-square test](https://www.datacamp.com/tutorial/chi-square-test-r){preview-link="false"} and [t-test](https://www.datacamp.com/tutorial/t-tests-r-tutorial){preview-link="false"}, among other...

-->

:::

# Ressources to gop further

* broom????





#########


## Log-normal distribution {.smaller}

$x \sim log\mathcal{N}(\mu, \sigma^2)$ if $log(x) \sim \mathcal{N}(\mu, \sigma^2)$

[We use the function [rnorm]{style="color:indianred;"} to randomly draw data from a normal distribution:]{style="font-size: 30px"}

::::: columns
::: {.column width="60%"}


```{r}
#| warning: false
dt1 <- data.frame(x = rlnorm(n = 100000, mean = 0, sd = 1))
dt2 <- data.frame(x = rlnorm(n = 100000, mean = 1, sd = 1))

ggplot() + 
  geom_density(data = dt1, aes(x = x), 
               color = "blue") + 
  geom_density(data = dt2, aes(x = x), 
               color = "green") + 
  scale_x_continuous(limits =c(0,20)) +
  theme_minimal()
```
:::

::: {.column width="40%"}

* The values are stricty positive.

* The variance increases when the mean increases. 

:::
:::::



# GLM?

binomial glm: cf p 254 book avec les pinguins



regression linéaire (en mnetionnant les GLM et les mixed-effect models)

include DHARma for model diagnostics?

Voir ici pour comparaison broom, modelr and DHARMa
https://chatgpt.com/share/680a764b-476c-8004-9a40-6c99c92a6d35



https://ericmarcon.github.io/Cours-R-Geeft/Regression_lineaire.html#2

