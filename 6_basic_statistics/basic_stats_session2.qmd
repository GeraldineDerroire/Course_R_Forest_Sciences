---
title: "Statistics with R - session 2 - Linear regressions"
author: G√©raldine Derroire
institute: Cirad - UnB
date: last-modified
format: 
  revealjs:
    theme: solarized
    output-location: fragment 
    slide-number: true
    preview-links: true
    chalkboard: true
    link-external-icon: true
    link-external-newwindow: true
    incremental: true
execute:
  echo: true   
  warning: true
  message: true 
  cache: true
editor: 
  markdown: 
    wrap: sentence
---

#

Let's load the packages that we will use today

```{r}
#| message: false
library(tidyverse)
library(ggeffects)
library(gtsummary)
library(ggstats)
library(lindia)
library(car)
library(AICcmodavg)
```


## Uses of regressions

* Study the relationship between several variables (without assuming causality)

* Study the effect of numerical or categorical variables on another variables 

* Use these relationships to predict the values of the modelled variable for new observations.


## Structure of a linear regression

A linear regression is a mathematical model of the relationship between:

* A **response variable**, also called *dependent variable*

* One or several **explanatory variables**, also called *predictors* or *independent variables*



# Simple linear regression

## Nouragues tree data {.scrollable}

[We will work on a modified version of Nouragues tree data from the package BIOMASS (available     [here](https://geraldinederroire.github.io/Course_R_Forest_Sciences/6_basic_statistics/data/data_statistics_2.RData){preview-link="false"}).]{style="font-size: 30px"}
[These data have been modified to exclude the non fully determined trees, and add the family.]{style="font-size: 30px"}

[*Heights and diameters of trees in two 1-ha plots from the Nouragues forest (French Guiana)*]{style="font-size: 30px"}


```{r}
load("data/data_statistics_2.RData")
dt_nou
```


## Relationship between D and H

[Let's explore the relationship between tree diameter and height graphically:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out.width: 80%
g <- ggplot(dt_nou, aes(x = D, y = H)) + 
  geom_point() 
g + geom_smooth(method = "lm", se = FALSE)
```


## Relationship between D and H

[We want to model the relationship between *D* (the explanatory variable) and *H* (the response variable).]{style="font-size: 30px"}

[[As our model has only one explanatory variable, it is called *simple* linear regression.]{style="font-size: 30px"}]{.fragment}


[[We want to model a tree dimension by another tree dimension. This is an *allometric model*. Allometric models are often used to predict a variable difficult to measure from a variable more broadly available.]{style="font-size: 30px"}]{.fragment}

::: notes
ex of an allometric model
:::


## Theory 

::: fragment
$$y = \alpha + \beta \times x + \epsilon$$
:::

::::: columns
::: {.column width="50%"}

* [y is the response variable]{style="font-size: 30px"}

* [x is the explanatory variable]{style="font-size: 30px"}

* [$\alpha$ is the **intercept** (the mean of Y when X = 0)]{style="font-size: 30px"}

* [$\beta$ is the **slope** of the relationship between X and Y]{style="font-size: 30px"}

* [$\epsilon$ is the **residual** of the model. $\epsilon \sim \mathcal N(0, \sigma^2)$]{style="font-size: 30px"}

:::

::: {.column width="50%"}

::: fragment
[$\alpha$, $\beta$ and $\epsilon$ are the **parameters** of the model]{style="font-size: 30px"}

[The model can also be written:]{style="font-size: 30px"}

$$y \sim \mathcal N(\alpha + \beta \times x,  \sigma^2)$$
:::
:::
:::::

::: notes
who want to write the equation of the model we want to fit on the blackboard?
:::


## Theory 

$$y \sim \mathcal N(\alpha + \beta \times x,  \sigma^2)$$

[The model predicts a probability density of Y  for any value of X normally distributed around the regression line.]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

![](lm.png){width="200%"}
[Source: [E. Marcon](https://ericmarcon.github.io/Cours-R-Geeft/Regression_lineaire.html#6){preview-link="false"}]{style="font-size: 22px"}
:::


::: {.column width="50%"}
* [The mean value of Y for any value of X is the black line : $\mu = \alpha + \beta \times x$]{style="font-size: 30px"}

* [The predicted probability density of Y are represented by the red dots: they are normally distributed around the mean with a variance $\sigma^2$]{style="font-size: 30px"}


:::
:::::

::: notes
read the fig as in 3D
:::


## Fitting a simple linear regression

[We use the function [lm]{style="color:indianred;"} to fit a linear regression:]{style="font-size: 30px"}

```{r}
mod_dh_1 <- lm(H ~ D, # formula
               data = dt_nou)
```

Syntax of the formula:

* response variable ~ explanatory variable(s)

* the intercept is implicit so *H ~ D* is equivalent to *H ~ 1 + D*

[[If we don't want an intercept, we have to write *H ~ 0 + D*. The regression will go through the origin (0,0).]{style="font-size: 30px"}]{.fragment}


## Interpreting the output {.smaller}


::::: columns
::: {.column width="50%"}

[$$y = \alpha + \beta \times x + \epsilon$$]{style="font-size: 30px"}

```{r}
summary(mod_dh_1)
```
:::

::: {.column width="50%"}
* [*Coefficients* give the estimates of **intercept** ($\alpha$) and the **slope** ($\beta$).]{style="font-size: 30px"}

  * *intercept*: the value of H for D=0 is 13.26
  
  * *slope*: for every increase of 1 cm DBH, increase of 0.34 m in height
  
  * [A t-test is performed to see if there are significant (significantly different from 0).]{style="font-size: 30px"}

* [*Residual standard error* is the estimate of the **standard deviation** of the residuals ($\sigma$).]{style="font-size: 30px"}

:::
::::::

::: notes
BUT don't use that to predict outside of the range of observed data
:::

## Interpreting the output {.smaller}


::::: columns
::: {.column width="50%"}

[$$y = \alpha + \beta \times x + \epsilon$$]{style="font-size: 30px"}

```{r}
summary(mod_dh_1)
```
:::

::: {.column width="50%"}
* [*R^2^ *(discussed after)]{style="font-size: 30px"}

* [The *F-statistic* test if the model is significantly different from a null model, *i.e.* if it explains a **significant amount of the variation** on the response variable.]{style="font-size: 30px"}

:::
::::::

::: notes
Null model : a model without explanatory variable
F-test for only one single explanatory variable is the same as the test for the explanatory variable
:::

## Coefficient of determination 

**R^2^** is the coefficient of determination: it represents the **proportion of variance of the response variable explained by the model**. The closer *R^2^* is to 1, the greater the explanatory power of the model.
*R^2^* therefore reflects the **goodness of fit** of the model.

[Adjusted *R^2^* penalise *R^2^* by the number of explanatory variables (not relevant when there is just one explanatory variable).]{.fragment}

## *R^2^* and *p-value*

* The **p-value** indicates the **significance of the effect** of the explanatory variable: this effect can be small yet highly significant => a small p-value is particularly important for hypothesis testing.


* The **R^2^** indicates the **goodness of fit** (how well the model fits the observed data) => a high *R^2^* is particularly important for prediction.


## Confidence interval

The **confidence interval** is the interval of confidence of the mean predicted value of y (so of $\mu$).

[[To calculate it, we can use the function [ggpredict]{style="color:indianred;"} from the package [ggeffects]{style="color:indianred;"}. The default give the confidence interval:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
# Get the prediction (default arguments gives the confidence interval)
preds_conf <- ggpredict(mod_dh_1, # model used for the prediction
                        terms = "D") # explanatory variable
slice_head(as.data.frame(preds_conf), n=3) # visualise the results
```
:::

::: notes
here the object is a dataframe and an object of class ggeffects
so to visualise it better, look at it as a dataframe
:::


## Confidence interval

[We can then plot the prediction mean and confidence interval:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out-width: 60%
gpred_1 <- ggplot(preds_conf, aes(x = x, y = predicted)) +
  geom_line(colour = "blue", linewidth = 1) + # predicted mean
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), # confidence interval
              fill = "blue", alpha = 0.2) + 
  geom_point(data = dt_nou, aes(x = D, y = H), alpha = 0.6) + # data
  labs(x = "D", y = "H", title = "Predicted H vs D") +
  theme_minimal()
gpred_1
```

## Prediction interval {.scrollable}

The **prediction interval** includes the uncertainty of the predicted mean (like the confidence interval) and the variability of individual prediction by taking into account the residual variance $\sigma^2$.
[It is therefore wider than the confidence interval and contains most of the observations.]{.fragment}

[[To calculate it, we can use the function [ggpredict]{style="color:indianred;"}, with the argument *interval = "prediction"*:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
# Get the prediction with prediction interval
preds_pred <- ggpredict(mod_dh_1, # model used for the prediction
                        terms = "D", # explanatory variable
                        interval = "prediction")
slice_head(as.data.frame(preds_pred), n=3) # visualise the results
```
:::


## Prediction interval

[We can then add the prediction interval to the plot:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out-width: 70%
gpred_1 <- gpred_1 +
  geom_ribbon(data = preds_pred, aes(ymin = conf.low, ymax = conf.high), 
              fill = "lightblue", alpha = 0.4) +
  theme_bw()
gpred_1
```



## Model validation

We need to check the model assumptions:

* normality of the residuals

* independence of the residuals

* constant variance of the residuals (homoscedasticity)

* linearity of the relationship between response and explanatory variables

## Homoscedasticity and independence of the residuals

::::: columns
::: {.column width="50%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 1)
```

:::


::: {.column width="50%"}

::: fragment
[‚ö†Ô∏è Here the residuals depends on the fitted values => this suggests that the relationship is not linear...]{style="font-size: 30px"}

[‚ö†Ô∏è The variance is not constant.]{style="font-size: 30px"}
:::

:::
:::::


## Normality of the residuals

::::: columns
::: {.column width="50%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 2)
```

:::


::: {.column width="50%"}
[[‚úÖ Here the residuals are reasonably normality distributed.]{style="font-size: 30px"}]{.fragment}
:::
:::::


## Leverage effect

::::: columns
::: {.column width="40%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 4)
```

:::


::: {.column width="60%"}

::: fragment
[‚ö†Ô∏èÔ∏è Some observations are highly influential.]{style="font-size: 30px"}

:::

:::
:::::

::: notes
for cook distance, several rule of thumbs

* D should be < 1

* D should be <4/n

:::


## Variable transformation

[Our model has problem of linearity and high leverage of some observations.]{style="font-size: 30px"}

::: fragment
[üí° A transformation of variables may help, let's try to take the log of D and H:]{style="font-size: 30px"}

$$log(H) = \alpha + \beta \times log(D)$$
:::


::: fragment

[which is equivalent to
$y = \alpha'x^\beta$ with $\alpha' = e^{\alpha}$
because]{style="font-size: 30px"}

[$log\big (\alpha'x^\beta \big) = log(\alpha') + \beta \times log(D)$]{style="font-size: 30px"}
:::

[[*This is a classical allometric model: it predicts that the height of the tree will increase less rapidly for trees with bigger DBH, which is consistent with what we expect.*]{style="font-size: 30px"}]{.fragment}


## Fitting the log-log model {.smaller}

```{r}
mod_dh_2 <- lm(log(H) ~ log(D), data = dt_nou)
summary(mod_dh_2)
```

## Model validation {.smaller}

::::: columns
::: {.column width="33%"}
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 1)
```

[[üòä We have improved the independance of residuals and homoscedasticity.]{style="font-size: 30px"}]{.fragment}
:::


::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 2)
```
:::
[[üòä The residuals are still (reasonably) normally distributed.]{style="font-size: 30px"}]{.fragment}
:::

::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 4)
```
:::

[[üòä We have considerably reduced the leverage effect.]{style="font-size: 30px"}]{.fragment}

:::
:::::



## Which model should we choose?

Here we would prefer the log-log model because:

* it is better regarding the model assumptions and reduced the leverage effect

* it has a slightly better R^2^

* it make more sense ecologically

[[‚ö†Ô∏è We will talk later about the AIC to compare models. We **cannot** use it here because the response variables are different (D *vs* log(D)).]{style="font-size: 30px"}]{.fragment}

::: notes
it makes more sense ecologically because of the shape of the relationship => for big trees, the increase in height for unit DBH is less
:::


## Presenting the results

[We have seen how to plot the results, but here we need to back-transform them to their original (non-logged) scale. By default, the function [ggpredict]{style="color:indianred;"} back-transform the predictions:]{style="font-size: 30px"}

```{r}
# Get predictions back on original scale
preds_conf <- ggpredict(mod_dh_2, terms = "D") # default gives the confidence interval

# plot the results
g_preds_2 <- ggplot(preds_conf, aes(x = x, y = predicted)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "blue", alpha = 0.2) +
  geom_point(data = dt_nou, aes(x = D, y = H), alpha = 0.6) +
  labs(x = "D", y = "H", title = "Predicted H vs D (log-log model, back-transformed)") +
  theme_minimal()
```

## Presenting the results

```{r}
#| warning: false
g_preds_2
```


## Presenting the results

[To add the prediction interval, we set the argument *interval* to "prediction":]{style="font-size: 30px"}

```{r}
# Get predictions back on original scale, with prediction interval
preds_pred <- ggpredict(mod_dh_2, terms = "D", interval = "prediction")  

# plot the results
g_preds_2 <- g_preds_2 +
  geom_ribbon(data = preds_pred,
              aes(ymin = conf.low, ymax = conf.high), 
              fill = "lightblue", alpha = 0.4)
```

## Presenting the results

```{r}
#| warning: false
g_preds_2
```

## Data transformation

Why transform the data?

* linearise a non-linear relationship (*eg*: quadratic relationship $y = \alpha + \beta_1 \times x + \beta_2 \times x^2$, square root or log-transformation...)

* improve normality and homoscedasticity of the residuals (*eg*: square root of log-transformation...)

* reduce the influence of outliers (*eg*: log-transformation...)


::: notes
show on the blackboard the shape of a quadratic and square root (as log) 

always explore the shape of the relationsing with simulated data, to understand what the transfo does
:::


## What if the predictor is categorical? 

[We now want to model D as a function of the family:]{style="font-size: 30px"} 

$$D \sim \mathcal N(\alpha_{fam},  \sigma^2)$$

[[Let's do it on the 3 most abundant families:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
fam3 <- dt_nou %>% count(family) %>% # get the 3 most abundant families
  arrange(desc(n)) %>%
  slice_head(n = 3) 

dt_nou_sub <- dt_nou %>% filter(family %in% fam3$family) # select them
```
:::

## What if the predictor is categorical? {.smaller}

[We can fit the model in the same way as for a numerical variable:]{style="font-size: 30px"}

::::: columns
::: {.column width="70%"}

```{r}
mod_dfam <- lm(D ~ family, data = dt_nou_sub)
summary(mod_dfam)
```

:::

::: {.column width="30%"}

[[As for the Anova, the intercept corresponds to the first level, and the other coefficients show the difference of each levels with the first level.]{style="font-size: 30px"}]{.fragment}

[[‚ö†Ô∏è Don't forget to check the model assumptions...]{style="font-size: 30px"}]{.fragment}

:::
:::::

::: notes
so here the mean D of the Chrysobalanaceae is 21

The fabaceae are significantly different with a mean diam of 34

we won't check the model assumption here, done as for a quanti variable
:::



## What if the predictor is categorical?

[The simple linear regression with a categorical explanatory variable is equivalent to the ANOVA:]{style="font-size: 30px"}

```{r}
aov_dfam <- aov(D ~ family, data = dt_nou_sub)
summary(aov_dfam)
```

[[‚ö†Ô∏è The t-test performed by *lm* evaluates if the first level is different from the other ones, whereas the Tukey test on an ANOVA consider all pairs of levels.]{style="font-size: 30px"}]{.fragment}

::: notes
see the value of the f-statistics and the overal p-value
::: 


# Multiple linear regression 

## Equation 

A multiple linear regression is a regression with several explanatory variables:

$$y \sim \mathcal N(\alpha + \beta_1 \times x_1 + \beta_2 \times x_2 + ... + \beta_n \times x_n,  \sigma^2)$$
which we can also write:


$$y \sim \mathcal N(\alpha + \sum_{i=1}^n\beta_n \times x_n ,  \sigma^2)$$
[üí° Explanatory variables can be numerical or categorical.]{.fragment}

## Fitting a multiple linear regression

Let's predict the best record time in bicycle races in Scotland (*time*), with the explanatory variables distance (*distance*) and altitude gained (*climb*), using the dataset *hills* from the MASS package:

::::: columns
::: {.column width="40%"}

```{r}
library(MASS)
```

:::

::: {.column width="60%"}
```{r}
data(hills)
detach("package:MASS", unload = TRUE)
summary(hills)
```

:::
:::::

::: notes
we detach the package to avoid select being masked

note that dist is in miles and varies between 2 and 28

climb is in meters and varies between 300 and 7500 feets.
:::

## Fitting a multiple linear regression

[We fit a multiple linear regression in the same way than a simple one, adding the explanatory variables with a [+]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
mod_hills <- lm(time ~ dist + climb, hills)
summary(mod_hills)
```


## Standardising explanatory variables

[To compare the effects of the explanatory variables, we can (but we don't necessarly need) standardise them using the function [scale]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
hills <- hills %>% 
  mutate(dist_sd = as.numeric(scale(dist)), # scale return a matrix so need to transform 
         climb_sd = as.numeric(scale(climb)))
```

[[Variables are scaled by subtracting the mean to each values and then dividing it by the standard deviation.]{style="font-size: 30px"}]{.fragment}
[[**This brings the variables to the same scale.** 
They now have a mean of 0 and a standard deviation of 1:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
hills %>% summarise(dist_mean = mean(dist_sd),
                    dist_sd = sd(dist_sd),
                    climb_mean = mean(climb_sd),
                    climb_sd = sd(climb_sd))
```
:::

## Standardising explanatory variables {.smaller}

::::: columns
::: {.column width="50%"}
```{r}
mod_hills2 <- lm(time ~ dist_sd + climb_sd, hills)
summary(mod_hills2)
```
:::

::: {.column width="50%"}
[[**Interpretation**: When the explanatory variable increases by one standard deviation, the response increases by the value of the coefficient.]{style="font-size: 30px"}]{.fragment}

[[We can therefore compare the effect of explanatory variables.]{style="font-size: 30px"}]{.fragment}

[[The intercept is the value of the response when all explanatory variables are taken at their mean.]{style="font-size: 30px"}]{.fragment}

::: notes
explain for dist and climb, and show the equation on the blackboard
:::
:::
:::::



## Model validation {.smaller}


::::: columns
::: {.column width="33%"}
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_hills2, which = 1)
```

:::


::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_hills2, which = 2)
```
:::
:::

::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_hills2, which = 4)
```
:::
:::
:::::

[[Here we see that a few observations have a strong effect, it would be worth inspecting these observations...]{style="font-size: 30px"}]{.fragment}

## Model validation


[For multiple linear regression, it is important to also check the effect of each explanatory variable individually by plotting the residuals against them.]{style="font-size: 30px"}
[[We use the function [gg_resX]{style="color:indianred;"} of the [lindia]{style="color:indianred;"} package:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
#| fig.width: 6
#| fig.height: 3
gg_resX(mod_hills2, ncol = 2)
```
:::

[[üí° the function [gg_diagnose]{style="color:indianred;"} does all the diagnostic plots.]{style="font-size: 30px"}]{.fragment}

::: notes
Here the diagnostic is reasonably ok, except for the effect of the influential observations
:::


## Collinearity

[For multiple linear regression we need to check the absence of collinearity of the explanatory variables.]{style="font-size: 30px"}

[**If an explanatory variable is a linear combination of the other, this impedes the model fitting.**]{style="font-size: 30px"}

::::: columns
::: {.column width="70%"}

::: fragment 
[We can look at the correlation between variables:]{style="font-size: 30px"}

```{r}
cor(hills$dist_sd, hills$climb_sd)
```
:::

::: fragment 
[We can also check the VIF (*variance inflation factor*), using the function [vif]{style="color:indianred;"} from the package [car]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
vif(mod_hills2)
```
:::

:::

::: {.column width="30%"}

[[‚ö†Ô∏è If the VIF of some explanatory variables is above 10 (or even 5), some explanatory variables should be excluded from the regression.]{style="font-size: 30px"}]{.fragment}

:::
:::::

::: notes
if the explanatory variables vary together, difficult to estimate their individual effects...

here we test the correlation between the standardised variables, but this would be the same for the non standardised...

there is some correlation

no strict rule for the VIF, this is context dependant. Check from 5
:::


## Presenting the results

[To represent graphically the effect of an explanatory variable, we can set the values of the other explanatory variables to a constant (for example their mean).]{style="font-size: 30px"}

[[Here we represent the effect of *dist_sd* setting *climb_sd* to its mean. By default, the function [ggpredict]{style="color:indianred;"} takes the values of explanatory variables not mentioned in the argument *terms* to their mean:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
# Get the prediction (default arguments gives the confidence interval)
pred_dist_conf <- ggpredict(mod_hills2, terms = "dist_sd")

# Make the plot
p_modhills2_dist <- ggplot(pred_dist_conf, aes(x = x, y = predicted)) + 
  geom_line(colour = "blue", linewidth = 1) + # predicted mean for climb_sd set to its mean
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), # confidence interval
              fill = "blue", alpha = 0.2) + 
  geom_point(data = hills, aes(x = dist_sd, y = time), alpha = 0.6) + # data 
  labs(x = "dist_sd")
```
:::

[[We could also represent *climb_sd* with *dist_sd* set to its mean.]{style="font-size: 25px"}]{.fragment}

::: notes
We can obviously do the same for climb_sd
:::

## Presenting the results

```{r}
p_modhills2_dist
```

## Presenting the results {.smaller}

[We could also represent the effect of *dist_sd* setting climb_sd at the mean, -1 sd, + 1 sd. For that, we give several explanatory variables to [ggpredict]{style="color:indianred;"}. All variables except the first one will be set to meaningful values:]{style="font-size: 30px"}

::: fragment
```{r}
# Get the prediction (default arguments gives the confidence interval)
pred_dist_conf <- ggpredict(mod_hills2, 
                            terms = c("dist_sd", # to predict with
                                      "climb_sd")) # will be set to a few meaningfull values
```
:::

[[*The values taken for climb_sd are factors, in the variable group.*]{style="font-size: 30px"}]{.fragment}

::::: columns
::: {.column width="40%"}

::: fragment
```{r}
pred_dist_conf
```
:::
:::

::: {.column width="60%"}

::: fragment
```{r}
slice_head(as.data.frame(pred_dist_conf), n = 3)
```
:::

:::
:::::

::: notes
meaningfull values: here -1, 0 and 1 sd
but we can specify some that we want (look at the help of ggpredic)
:::


## Presenting the results {.smaller}

```{r}
#| out.width: 80%
# Make the plot
p_modhills2_dist <- ggplot(pred_dist_conf, 
                           aes(x = x, y = predicted)) + 
  geom_line(aes(colour = group), linewidth = 1) +  # a color per set values of climb_sd 
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, colour = group), alpha = 0.2) + 
  geom_point(data = hills, aes(x = dist_sd, y = time, # data
                               size = climb_sd), # size proportional to climb_sd
             alpha = 0.6)  +
  labs(x = "dist_sd")
p_modhills2_dist
```

::: notes
we add the group (the valus of clim_sd) as a colour for the lines and the ribbons
:::

## Presenting the results

::::: columns
::: {.column width="50%"}
[If we have many explanatory variables, we can plot all the coefficients with their confidence interval, using the function 
[ggcoef_model]{style="color:indianred;"} of the package [ggstats]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
ggcoef_model(mod_hills2)
```

:::

::: {.column width="50%"}

::: fragment
[or present a table of the coefficients, using [tbl_regression]{style="color:indianred;"} from the package [gtsummary]{style="color:indianred;"}]{style="font-size: 30px"}

```{r}
tbl_regression(mod_hills2, 
               intercept = TRUE)
```
:::
:::
:::::


## Interaction {.smaller}

[We can add an **interaction** term in the model to test if the effect of an explanatory variable depend of the value taken by another explanatory variable:]{style="font-size: 30px"}

$$y = \alpha + \beta_1 x_1 + \beta_2  x_2 + \beta_{12}  x_1 x_2 + \epsilon$$

[[which is equivalent to]{style="font-size: 30px"}]{.fragment}

::: fragment

$$y = \alpha + (\beta_1+\beta_{12} x_2) \times x_1 + \beta_2  x_2+ \epsilon$$
:::

[[and]{style="font-size: 30px"}]{.fragment}

::: fragment

$$y = \alpha + \beta_1  x_1+ (\beta_2+\beta_{12} x_1) \times x_2 +  \epsilon$$
:::

[[$\beta_{12}$ is the coefficient of the interaction. It represent the effect of $x_2$ on the relationship between y and $x_1$, and the effect of $x_1$ on the relationship between y and $x_2$.]{style="font-size: 30px"}]{.fragment}

::: notes
here we carry on with our model with two numerical variables, but could be for 1 cat and one numerical

as alaways epsilon is N(0, sigma2)
::: 

## Interaction {.smaller}

[To specify an interaction, we use [*]{style="color:indianred;"} in the formula:]{style="font-size: 30px"}

```{r}
mod_hills_inter <- lm(time ~ dist_sd * climb_sd, hills)
summary(mod_hills_inter)
```

## Visualisation of a model with interaction

[We can make a graph of the predictions as we did for the model without interaction:]{style="font-size: 30px"}

```{r}
# Get the prediction (default arguments gives the confidence interval)
pred_inter_dist_conf <- ggpredict(mod_hills_inter, 
                                  terms = c("dist_sd", # to predict with
                                            "climb_sd")) # will be set to a few meaningfull values

# Make the plot
p_modinter_dist <- ggplot(pred_inter_dist_conf, 
                           aes(x = x, y = predicted)) + 
  geom_line(aes(colour = group), linewidth = 1) +  # a color per set values of climb_sd 
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, colour = group), alpha = 0.2) + 
  geom_point(data = hills, aes(x = dist_sd, y = time, # data
                               size = climb_sd), # size proportional to climb_sd
             alpha = 0.6)  +
  labs(x = "dist_sd")
```

## Visualisation of a model with interaction

```{r}
p_modinter_dist
```

[[The prediction lines are no longer parallel, because of the interaction term.]{style="font-size: 30px"}]{.fragment}

## Model comparison

[To compare several models, we can use the AIC (Akaike information criterium).]{style="font-size: 30px"}

[The AIC take into account the likelihood of the model while penalising a more complex model (*i.e.* a model with more parameters) to avoid over-fitting.]{style="font-size: 30px"}

[[To compare models using the AIC, they need to:]{style="font-size: 30px"}]{.fragment}

* [have the same response variable (not possible to compare a model with a transformation of the response variable with a model without transformation)]{style="font-size: 30px"}

* [be fitted on the same data (so be careful with NAs)]{style="font-size: 30px"}

[[**The model with the lowest AIC is the best model** (we usually consider a model difference of at least 2).]{style="font-size: 30px"}]{.fragment}


::: notes
explain overfitting: when a model fit the data too closely, capturing random noise and fluctuation in addition to the true underlying pattern

the value of AIC has no meaning, it needs to be used to compare models
:::


## Model comparison {.smaller}

[Let's compare our models with and without interactions, and models with only one explanatory variable. 
We use the function [aictab]{style="color:indianred;"} of the package [AICcmodavg]{style="color:indianred;"}:]{style="font-size: 30px"}

::::: columns
::: {.column width="70%"}

```{r}
# fit two additional simple linear models
mod_hills_dist <- lm(time ~ dist_sd, data = hills) 
mod_hills_climb <- lm(time ~ climb_sd, data = hills)
# make a list with all the models
list_mod <- list(hills_dist = mod_hills_dist, 
                 hills_climb = mod_hills_climb, 
                 hills2 = mod_hills2, 
                 hills_inter = mod_hills_inter)
# compare the AIC
aictab(list_mod)
```
:::

::: {.column width="30%"}
[[The model with interaction has the lowest AIC, it is therefore the best model.]{style="font-size: 30px"}]{.fragment}
:::
:::::

::: notes
here compute the AICc because we have a small number of observations

we get the K: number of paramaters (count them, including the sigma)

the LL is the log-likelihood
:::



# To go further

## What if the observations are not independent? 

An linear regression assumes that all observations are **independent**.

When observations are grouped, they are not independent (*e.g.* several observations on a same site, on a same individual, in a same year...).

* We can include the group as a categorical explanatory variable. This evaluate the effect of each group.

* When the number of groups is too high and we are not interested in the effect of each group, we can use a **linear mixed model**.

## Linear mixed model {.smaller}

[Linear mixed models have a hierarchical structure.]{style="font-size: 30px"}

::: fragment

[One or more of their coefficients (the intercept or the response to explanatory variable) varies between groups *j*:]{style="font-size: 30px"}

$$y_k \sim \mathcal N(\alpha_{j[k]} + \beta_1 \times x_1 + \beta_2 \times x_2 + ... ,  \sigma^2)$$
:::

::: fragment

[and this variation is modelled by a distribution:]{style="font-size: 30px"}

$$\alpha_j \sim \mathcal N(\mu_{\alpha}, \sigma^2_{\alpha})$$

[*NB: here we represent the effect of the group only on the intercept, but we can include this effect on the other coefficients as well.*]{style="font-size: 22px"}

:::

[[üí° See [here](https://meghan.rbind.io/blog/2022-06-28-a-beginners-guide-to-mixed-effects-models/){preview-link="false"} for a detailed explanation of linear mixed model.]{style="font-size: 30px"}]{.fragment}



## What if the data are not normally distributed?

Some data are by nature not normally distributed, so we cannot model them with a normal distribution.

* count data (*e.g.*: number of individuals of a species in a plot, number of birds heard in a given time) can be modelled with a **Poisson** distribution

* success/failure data (*e.g*: presence/absence of a species in a plot, proportion of tree browsed in a plot) can be modelled with a **binomial** distribution

[[To know more about these distributions: see the Wikipedia pages, and the [distribution zoo](https://ben18785.shinyapps.io/distribution-zoo/){preview-link="false"}.]{style="font-size: 25px"}]{.fragment}


## Generalised linear models

*Generalised linear models* (**GLM**) are an extension of linear models allowing different distributions of the response variable.

[[They have 3 component:]{style="font-size: 30px"}]{.fragment}

* [a **linear predictor**: a linear combination of explanatory variables]{style="font-size: 30px"}

* [a **distribution** of individual responses around their mean]{style="font-size: 30px"}

* [a **link function** between the predictor (defined on $\mathbb{R}$) and the mean response (not necessarily defined on $\mathbb{R}$)]{style="font-size: 30px"}

[[In the linear model seen before, the distribution is *Normal* and the link function the *identity*.]{style="font-size: 25px"}]{.fragment}


## Logistic regression

[The logistic regression is a **GLM** with a **binomial distribution** and a **logit link function**. It models a probability of success.]{style="font-size: 30px"}

* [The binomial distribution $\mathcal{B}(n, p)$ represents the number of successes when performing ***n*** independent experiment having a constant probability of success ***p***. ]{style="font-size: 30px"}[[It is a discrete distribution taking only **positive integer values**.]{style="font-size: 30px"}]{.fragment}

* [The logit link function links the modelled probability of success (which is between 0 and 1) to the linear predictor.]{style="font-size: 30px"}

[[The Bernoulli distribution is a special case of a binomial when n=1.]{style="font-size: 25px"}]{.fragment}

[[üí° See [here](https://www.geo.fu-berlin.de/en/v/soga-r/Basics-of-statistics/Logistic-Regression/Logistic-Regression-in-R---An-Example/index.html){preview-link="false"} for a detailed explanation of the logistic regression.]{style="font-size: 30px"}]{.fragment}

::: notes
logit(p) = alpha + beta_1*x_1 +... beta_n*x_n

avec logit(p) = log (p/ (1-p))
:::

## Poisson

[The Poisson regression is a **GLM** with a **Poisson distribution** and a **logarithm link function**. It models the number of observations of an event in a defined sampling unit.]{style="font-size: 30px"}

* [The Poisson distribution $\mathcal{P}(\lambda)$ represents the mean number of observations **$\lambda$** in a sampling unit.]{style="font-size: 30px"}

* [The logarithm link function links **$\lambda$** (which is positive) to the linear predictor.]{style="font-size: 30px"} 

[[üí° See [here](https://www.dataquest.io/blog/tutorial-poisson-regression-in-r/){preview-link="false"} and [here](https://rpubs.com/Julian_Sampedro/1047952){preview-link="false"} for detailed explanations of the Poisson regression.]{style="font-size: 30px"}]{.fragment}

::: notes
log(mu) = alpha + beta_1*x_1 +... beta_n*x_n
:::

# Acknowledgments

::: {.nonincremental}

* Marchand P. *Analyses et mod√©lisation des donn√©es √©cologiques* [in French](https://pmarchand1.github.io/ECL7102/){preview-link="false"}

* Marcon E. *cours-R-Geeft* [in French](https://ericmarcon.github.io/Cours-R-Geeft/){preview-link="false"}

* Lamarange J. *Guide-R Guide pour l'analyse de donn√©es d'enqu√™tes avec R* [in French](https://larmarange.github.io/guide-R/){preview-link="false"}


:::

# Ressources

::: {.nonincremental}

* Lilja D.J. & Linse G.M. *Linear regression using R - An introduction to data modeling* [in English](https://conservancy.umn.edu/server/api/core/bitstreams/2a91814c-7194-4f39-aefb-c2babb7fb582/content){preview-link="false"}

* DataCamp ressources on linear regression in Portuguese  [here](https://www.datacamp.com/pt/tutorial/linear-regression-R){preview-link="false"}, [here](https://www.datacamp.com/pt/tutorial/simple-linear-regression){preview-link="false"} and [here](https://www.datacamp.com/pt/tutorial/multiple-linear-regression-r-tutorial){preview-link="false"}

:::

# Ressources to go further

* DHARMa package (F. Hartig) for model diagnostic with GLM [vignette](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html){preview-link="false"}



