---
title: "Statistics with R - session 2"
author: G√©raldine Derroire
institute: Cirad - UnB
date: last-modified
format: 
  revealjs:
    theme: solarized
    output-location: fragment 
    slide-number: true
    preview-links: true
    chalkboard: true
    link-external-icon: true
    link-external-newwindow: true
    incremental: true
execute:
  echo: true   
  warning: true
  message: true 
  cache: true
editor: 
  markdown: 
    wrap: sentence
---

#

Let's load the packages that we will use today

```{r}
#| message: false
library(tidyverse)
library(GGally)
library(ggeffects)
library(gtsummary)
library(lindia)
library(car)
```

::: notes
GGally to see pair-correlation
:::

#

Last session, we saw how to test for differences between variables.

Today we are going to look at **relationships between variables**.


# Covariance and correlation

## Covariance {.scrollable}

We have seen that the variance of a variable *x* is the mean of squared deviations from the mean:

$$\sigma^2_x = \frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n}$$

::: fragment

[The covariance between two numerical variables *x* and *y* is the mean of the product of the deviations of *x* and *y* from their respective mean:]{style="font-size: 30px"}

$$cov_{xy} = \frac{\sum_{i=1}^n (x_i-\bar{x}) \times (y_i-\bar{y})}{n}$$

:::

## Covariance

We use the function [cov]{style="color:indianred;"} to get the covariance between two variables.

Let's test this with the iris data set, which you graphically explore in a last exercise. We focus on the species *versicolor*:

```{r}
data(iris)
iris <- as_tibble(iris)
iris_vers <- iris %>% filter(Species == "versicolor")
cov(iris_vers$Petal.Length, iris_vers$Petal.Width)
```
[[‚ö†Ô∏è The unit of the covariance is the product of the unit of x and of y...]{style="font-size: 30px"}]{.fragment}


## Correlation of Pearson

[To help with the interpretation of the covariance, we standardise the covariance by the product of the standard deviations.]{style="font-size: 30px"}

[This is the **coefficient of correlation of Pearson**, which is between -1 and 1:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

* [a coefficient = -1 (respectively 1) means that the variables are perfectly negatively (respectively positively) correlated]{style="font-size: 30px"}

* [a coefficient = 0 shows an absence of correlation]{style="font-size: 30px"}
:::

::: {.column width="50%"}

[[We use the function [cor]{style="color:indianred;"} to get the correlation between two variables (the default method is Pearson correlation).]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
cor(iris_vers$Petal.Length, 
    iris_vers$Petal.Width)
```
:::

[[*There is a strong positive correlation.*]{style="font-size: 30px"}]{.fragment}

:::
:::::


::: notes
So here strong positive correlation
:::

## Correlation of Spearman

[Pearson correlation makes the hypothesis that the relationship is linear.]{style="font-size: 30px"}

[If this is not the case, we can use the *rank correlation of Spearman*, by specifying the method:]{style="font-size: 30px"}

```{r}
cor(iris_vers$Petal.Length, iris_vers$Petal.Width,
    method = "spearman")
```
::: notes
here, not much difference
:::

## Test of correlation

[To test if the correlation is significant, we do a test of correlation with the function [cor.test]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
cor.test(iris_vers$Petal.Length, iris_vers$Petal.Width,
         method = "pearson")
```

[[*The correlation is significant.*]{style="font-size: 30px"}]{.fragment}

## Visualise correlations 

[Pearson correlations between multiple pairs of variables can be visualised using the function [ggpairs]{style="color:indianred;"} of the package [GGally]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
#| out.width: 80%
ggpairs(iris_vers, columns = c("Sepal.Length", "Sepal.Width",
                               "Petal.Length", "Petal.Width"))
# this is equivalent to 
 # ggpairs(iris_vers, columns = 1:4)
```

::: notes
we get the correlation coef and its significance

more complicated if we want other type of correlation than Pearson
:::


# Simple linear regression

## Nouragues tree data {.scrollable}

[We will work on a modified version of Nouragues tree data from the package BIOMASS (available     [here](https://geraldinederroire.github.io/Course_R_Forest_Sciences/6_basic_statistics/data/data_statistics_2.RData){preview-link="false"}).]{style="font-size: 30px"}
[These data have been modified to exclude the non fully determined trees, and add the family.]{style="font-size: 30px"}

[*Heights and diameters of trees in two 1-ha plots from the Nouragues forest (French Guiana)*]{style="font-size: 30px"}


```{r}
load("data/data_statistics_2.RData")
dt_nou
```


<!-- could be plant growth rate as a function of soil humidity:
https://pmarchand1.github.io/ECL7102/notes_cours/6-Regression_lineaire.html

or maybe the data from the Nouragues
=> modelling heigth ~ D
=> then talking about log transfor
=> selection of the best model
! voir si je peux aussi m'en servir pour tester avec Category? Famille (√† importer avec Biomass) ou plot?
=> tester puis rajouter interaction entre D et famille?
=> et effet al√©atoire famille?
--> 

## Relationship between D and H

[Let's explore the relationship between tree diameter and height graphically:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out.width: 80%
g <- ggplot(dt_nou, aes(x = D, y = H)) + 
  geom_point() 
g + geom_smooth(method = "lm", se = FALSE)
```


## Structure of a linear regression

A linear regression is a model of the mathematical relationship between:

* A **response variable**, also called *dependent variable*

* One or several **explanatory variables**, also called *predictors* or *independent variables*

[[In our case, we want to model the relationship between *D* (the explanatory variable) and *H* (the response variable).]{style="font-size: 30px"}]{.fragment}

[[As our model has only one explanatory variable, it is called *simple* linear regression.]{style="font-size: 30px"}]{.fragment}



## Uses of regressions

<!-- TO DO: start the lesson with this if I move the correlation in the previous lesson?-->

* Study the relationship between several variables (without assuming causality)

* Study the effect of numerical or categorical variables on another variables 

* Use these relationships to predict the values of the modelled variable for new observations.

[[In our case, we want to model a tree dimension by another tree dimension. This is an *allometric model*. Allometric models are often used to predict a variable difficult to measure from a variable more broadly available]{style="font-size: 30px"}]{.fragment}


## Theory 

$$y = \alpha + \beta \times x + \epsilon$$

::::: columns
::: {.column width="50%"}

* [x is the response variable]{style="font-size: 30px"}

* [y is the explanatory variable]{style="font-size: 30px"}

* [$\alpha$ is the **intercept** (the mean of Y when X = 0)]{style="font-size: 30px"}

* [$\beta$ is the **slope** of the relationship between X and Y]{style="font-size: 30px"}

* [$\epsilon$ is the **residual** of the model. $\epsilon \sim \mathcal N(0, \sigma^2)$]{style="font-size: 30px"}

* [$\alpha$, $\beta$ and $\epsilon$ are the **parameters** of the model]{style="font-size: 30px"}
:::

::: {.column width="50%"}

::: fragment
[The model can also be written:]{style="font-size: 30px"}

$$y \sim \mathcal N(\alpha + \beta \times x,  \sigma^2)$$
:::
:::
:::::

::: notes
who want to write the equation of the model we want to fit on the blackboard?
:::


## Theory 

$$y \sim \mathcal N(\alpha + \beta \times x,  \sigma^2)$$

[The model predicts a probability density of Y  for any value of X normally distributed around the regression line.]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

![](lm.png){width="200%"}
[Source: [E. Marcon](https://ericmarcon.github.io/Cours-R-Geeft/Regression_lineaire.html#6){preview-link="false"}]{style="font-size: 30px"}
:::


::: {.column width="50%"}
* [The mean value of Y for any value of X is the black line : $\mu = \alpha + \beta \times x$]{style="font-size: 30px"}

* [The predicted probability density of Y are represented by the red dots: they are normally distributed around the mean with a variance $\sigma^2$]{style="font-size: 30px"}


:::
:::::

::: notes
read the fig as in 3D
:::


## Fitting a simple linear regression

[We use the function [lm]{style="color:indianred;"} to fit a linear regression:]{style="font-size: 30px"}

```{r}
mod_dh_1 <- lm(H ~ D, # formula
               data = dt_nou)
```

Syntax of the formula:

* response variable ~ explanatory variable(s)

* the intercept is implicit so *H ~ D* is equivalent to *H ~ 1 + D*

[[If we don't want an intercept, we have to write *H ~ 0 + D*. The regression will go through the origin (0,0).]{style="font-size: 30px"}]{.fragment}


## Interpreting the output {.smaller}


::::: columns
::: {.column width="50%"}

[$$y = \alpha + \beta \times x + \epsilon$$]{style="font-size: 30px"}

```{r}
summary(mod_dh_1)
```
:::

::: {.column width="50%"}
* [*Coefficients* give the estimates of **intercept** ($\alpha$) and the **slope** ($\beta$).]{style="font-size: 30px"}

  * *intercept*: the value of H for D=0 is 0.325
  
  * *slope*: for every increase of 1 cm DBH, increase of 0.32 m in height
  
  * [A t-test is performed to see if there are significant (significantly different from 0).]{style="font-size: 30px"}

* [*Residual standard error* is the estimate of the **standard deviation** of the residuals ($\sigma$).]{style="font-size: 30px"}

:::
::::::

::: notes
BUT don't use that to predict outside of the range of observed data
:::

## Interpreting the output {.smaller}


::::: columns
::: {.column width="50%"}

[$$y = \alpha + \beta \times x + \epsilon$$]{style="font-size: 30px"}

```{r}
summary(mod_dh_1)
```
:::

::: {.column width="50%"}
* [*R^2^ *(discussed after)]{style="font-size: 30px"}

* [The *F-statistic* test if the model is significantly different from a null model, *i.e.* if it explains a **significant amount of the variation** on the response variable.]{style="font-size: 30px"}

:::
::::::

::: notes
Null model : a model without explanatory variable
F-test for only one single explanatory variable is the same as the test for the explanatory variable
:::

## Coefficient of determination 

The **R^2^** is the coefficient of determination: it represents the **fraction of variance explained** by the model. The closer the *R^2^* is to 1, the greater the explanatory power of the model.

[The adjusted *R^2^* penalise the *R^2^* by the number of explanatory variables (not relevant when there is just one explanatory variable).]{.fragment}

## *R^2^* and *p-value*

* The **p-value** indicates **significance of the effect** of the explanatory variable: this effect can be small yet highly significant => a small p-value is particularly important for hypothesis testing.


* The **R^2^** indicates the **goodness of fit** (how well the model fits the observed data) => a high *R^2^* is particularly important for prediction.


## Confidence interval

The **confidence interval** is the interval of confidence of the mean predicted value of y (so of $\mu$).

[[To calculate it, we can use the function [predict]{style="color:indianred;"}, with the argument *interval = "confidence"*, on new data that we create for the prediction:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
new <- tibble(D = seq(from = 10, to = 160, by= 10)) # create new data
new_conf <- predict(mod_dh_1, newdata = new, 
                    interval = "confidence") # calculate the confidence interval
new_conf <- bind_cols(new, new_conf) # combine the two
slice_head(new_conf, n=3) # visualise to check
```
:::

::: notes
why do we create new data? to do less predictions than if we were using the observed D 

no need to take too many points because we have at line
:::

## Confidence interval

[We can then plot the confidence interval:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out-width: 70%
gpred <- ggplot(new_conf, aes(x= D)) +
  geom_line(aes(y = fit), color = "blue") + # prediction mean
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, color = "darkgrey") + # CI
  geom_point(data = dt_nou, aes(x = D, y = H), alpha = 0.3) + # observations
  theme_bw()
gpred
```




## Prediction interval {.scrollable}

The **prediction interval** includes the uncertainty of the predicted mean (like the confidence interval) and the variability of individual prediction by taking into account the residual variance $\sigma^2$.
[It is therefore wider than the confidence interval and contains most of the observations.]{.fragment}

[[To calculate it, we can use the function [predict]{style="color:indianred;"}, with the argument *interval = "prediction"*, on new data that we created for the prediction:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
new_pred <- predict(mod_dh_1, newdata = new,
                    interval = "prediction") # calculate the prediction interval
new_pred <- bind_cols(new, new_pred)
slice_head(new_pred) # visualise to check
```
:::

## Prediction interval

[We can then add the prediction interval to the plot:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out-width: 70%
gpred <- gpred +
  geom_ribbon(data = new_pred, aes(ymin = lwr, ymax = upr), 
              alpha = 0.3, color = "lightgrey") +
  theme_bw()
gpred
```



## Model validation

We need to check the model assumptions:

* normality of the residuals

* independence of the residuals

* constant variance of the residuals (homoscedasticity)

* linearity of the relationship between response and explanatory variables

## Homoscedasticity and independence of the residuals

::::: columns
::: {.column width="50%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 1)
```

:::


::: {.column width="50%"}

::: fragment
[‚ö†Ô∏è Here the residuals depends on the fitted values => this suggests that the relationship is not linear...]{style="font-size: 30px"}

[‚ö†Ô∏è The variance is not constant.]{style="font-size: 30px"}
:::

:::
:::::


## Normality of the residuals

::::: columns
::: {.column width="50%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 2)
```

:::


::: {.column width="50%"}
[[‚úÖ Here the residuals are reasonably normality distributed.]{style="font-size: 30px"}]{.fragment}
:::
:::::


## Leverage effect

::::: columns
::: {.column width="40%"}

```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_1, which = 4)
```

:::


::: {.column width="60%"}

::: fragment
[‚ö†Ô∏èÔ∏è Some observations are highly influential.]{style="font-size: 30px"}

:::

:::
:::::


## Variable transformation

[Our model has problem of linearity and high leverage of some observations.]{style="font-size: 30px"}

::: fragment
[üí° A transformation of variables may help, let's try to take the log of D and H:]{style="font-size: 30px"}

$$log(H) = \alpha + \beta \times log(D)$$
:::


::: fragment

[which is equivalent to
$y = \alpha'x^\beta$ with $\alpha' = e^{\alpha}$
because]{style="font-size: 30px"}

[$log\big (\alpha'x^\beta \big) = log(\alpha') + \beta \times log(D)$]{style="font-size: 30px"}
:::

[[*This is a classical allometric model: it predicts that the height of the tree will increase less rapidly for trees with bigger DBH, which is consistent with what we expect.*]{style="font-size: 30px"}]{.fragment}


## Fitting the log-log model {.smaller}

```{r}
mod_dh_2 <- lm(log(H) ~ log(D), data = dt_nou)
summary(mod_dh_2)
```

## Model validation {.smaller}

::::: columns
::: {.column width="33%"}
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 1)
```

[[üòä We have improved the independance of residuals and homoscedasticity.]{style="font-size: 30px"}]{.fragment}
:::


::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 2)
```
:::
[[üòä The residuals are still normally distributed.]{style="font-size: 30px"}]{.fragment}
:::

::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_dh_2, which = 4)
```
:::

[[üòä We have considerably reduced the leverage effect.]{style="font-size: 30px"}]{.fragment}

:::
:::::



## Which model should we choose?

Here we would prefer the log-log model because:

* it is better regarding the model assumptions and reduced the leverage effect

* it has a slightly better R^2^

* it make more sense ecologically

[[‚ö†Ô∏è We will talk later about the AIC to compare models. We **cannot** use it here because the response variables are different (D *vs* log(D)).]{style="font-size: 30px"}]{.fragment}


## Presenting the results

[We have seen how to plot the results, but here we need to back-transform them to their original (non-logged) scale. We use the function [ggpredict]{style="color:indianred;"} from the package [ggeffects]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
# Get predictions back on original scale
preds <- ggpredict(mod_dh_2, terms = "D") # default gives the confidence interval

# plot the results
g_preds_2 <- ggplot(preds, aes(x = x, y = predicted)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "blue", alpha = 0.2) +
  geom_point(data = dt_nou, aes(x = D, y = H), alpha = 0.6) +
  labs(x = "D", y = "H", title = "Predicted H vs D (log-log model, back-transformed)") +
  theme_minimal()
```

## Presenting the results

```{r}
#| warning: false
g_preds_2
```


## Presenting the results

[To add the prediction interval, we set the argument *interval* to "prediction":]{style="font-size: 30px"}

```{r}
# Get predictions back on original scale, with prediction interval
preds_pred <- ggeffects::ggpredict(mod_dh_2, terms = "D", interval = "prediction")  

# plot the results
g_preds_2 <- g_preds_2 +
  geom_ribbon(data = preds_pred,
              aes(ymin = conf.low, ymax = conf.high), 
              fill = "darkgrey", alpha = 0.2)
```

## Presenting the results

```{r}
#| warning: false
g_preds_2
```


## Data transformation

Why transform the data?

* linearise a non-linear relationship (*eg*: quadratic relationship $y = \alpha + \beta \times x^2$, or log-transformation)

* improve normality and homoscedasticity of the residuals (*eg*: square rot of log-transformation...)

<!--TO CHECK AND COMPLEMENT -->


::: notes
show on the blackboard the shape of a quadratic and square root

always explore the shape of the relationsing with simulated data, to understand what the transfo does
:::


## What if the predictor is categorical? 

[We now want to model D as a function of the family:]{style="font-size: 30px"} 

$$D \sim \mathcal N(\alpha_{fam},  \sigma^2)$$

[[Let's do it on the 3 most abundant families:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
fam3 <- dt_nou %>% count(family) %>% # get the 3 most abundant families
  arrange(desc(n)) %>%
  slice_head(n = 3) %>%
  select(family)

dt_nou_sub <- dt_nou %>% filter(family %in% fam3$family) # select them
```
:::

## What if the predictor is categorical? {.smaller}

[We can fit the model in the same way as for a numerical variable:]{style="font-size: 30px"}

::::: columns
::: {.column width="70%"}

```{r}
mod_dfam <- lm(D ~ family, data = dt_nou_sub)
summary(mod_dfam)
```

:::

::: {.column width="30%"}

[[As for the Anova, the intercept corresponds to the first level, and the other coefficients show the difference of each levels with the first level.]{style="font-size: 30px"}]{.fragment}

[[‚ö†Ô∏è Don't forget to check the model assumptions...]{style="font-size: 30px"}]{.fragment}

:::
:::::

::: notes
so here the D of mean D of the Chrysobalanaceae is 21

The fabaceae are significantly different with a mean diam of 34

we won't check the model assumption here, done as for a quanti variable
:::



## What if the predictor is categorical?

[The simple linear regression with a categorical explanatory variable is equivalent to the ANOVA:]{style="font-size: 30px"}

```{r}
aov_dfam <- aov(D ~ family, data = dt_nou_sub)
summary(aov_dfam)
```

[[‚ö†Ô∏è The t-test performed by *lm* evaluates if the first level is different from the other ones, whereas the Tukey test on an ANOVA consider all pairs of levels.]{style="font-size: 30px"}]{.fragment}

::: notes
see the value of the f-statistics and the overal p-value
::: 


<!--
# Multiple linear regression - One quanti and one quali

maybe go directly to several quanti... so I can talk about collinearity and selection

follow marchand et Eric 

voir si je parle de emmean ou plut√¥t de ggstats::ggcoef_model() dans Lamaranche

## Data

je pourrais continuer avec les H/diam mais qu'un seul pr√©dicteur continue...


-->

# Multiple linear regression 

## Equation 

A multiple linear regression is a regression with several explanatory variables:

$$y \sim \mathcal N(\alpha + \beta_1 \times x_1 + \beta_2 \times x_2 + ... + \beta_n \times x_n,  \sigma^2)$$
which we can also write:


$$y \sim \mathcal N(\alpha + \sum_{i=1}^n\beta_n \times x_n ,  \sigma^2)$$
[üí° Explanatory variables can be numerical or categorical.]{.fragment}

## Fitting a multiple linear regression

Let's predict the best record time in bicycle races in Scotland (*time*), with the explanatory variables distance (*distance*) and altitude gained (*climb*), using the dataset *hills* from the MASS package:

::::: columns
::: {.column width="40%"}

```{r}
library(MASS)
```

:::

::: {.column width="60%"}
```{r}
data(hills)
detach("package:MASS", unload = TRUE)
summary(hills)
```

:::
:::::

::: notes
we detach the package to avoid select being masked

note that dist is in miles and varies between 2 and 28

climb is in meters and varies between 300 and 7500 feets.
:::

## Fitting a multiple linear regression

[We fit a multiple linear regression in the same way than a simple one:]{style="font-size: 30px"}

```{r}
mod_hills <- lm(time ~ dist + climb, hills)
summary(mod_hills)
```


## Standardising explanatory variables

<!-- TO DO check if there is a way to scale with tidyverse and avoid the [,]-->

[To compare the effects of the explanatory variables, we can standardise them using the function [scale]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
hills <- hills %>% 
  mutate(dist_sd = scale(dist)[,1],
         climb_sd = scale(climb)[,1])
```

[[Variables are scaled by subtracting the mean to each values and then dividing it by the standard deviation.]{style="font-size: 30px"}]{.fragment}
[[**This brings the variables to the same scale.** 
They now have a mean of 0 and a standard deviation of 1:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
hills %>% summarise(dist_mean = mean(dist_sd),
                    dist_sd = sd(dist_sd),
                    climb_mean = mean(climb_sd),
                    climb_sd = sd(climb_sd))
```
:::

## Standardising explanatory variables {.smaller}

::::: columns
::: {.column width="50%"}
```{r}
mod_hills2 <- lm(time ~ dist_sd + climb_sd, hills)
summary(mod_hills2)
```
:::

::: {.column width="50%"}
[[**Interpretation**: When the explanatory variable increases by one standard deviation, the response increases by the value of the coefficient.]{style="font-size: 30px"}]{.fragment}

[[We can therefore compare the effect of explanatory variables.]{style="font-size: 30px"}]{.fragment}

[[The intercept is the value of the response when all explanatory variables are taken at their mean.]{style="font-size: 30px"}]{.fragment}

::: notes
explain for dist and climb, and show the equation on the blackboard
:::
:::
:::::



## Model validation {.smaller}


::::: columns
::: {.column width="33%"}
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_hills2, which = 1)
```

:::


::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_hills2, which = 2)
```
:::
:::

::: {.column width="33%"}
::: fragment
```{r}
#| fig-width: 4
#| fig-height: 4
plot(mod_hills2, which = 4)
```
:::
:::
:::::

[[Here we see that a few observations have a strong effect, it would be worth inspecting these observations...]{style="font-size: 30px"}]{.fragment}

## Model validation


[For multiple linear regression, it is important to also check the effect of each explanatory variable individually by plotting the residuals against them.]{style="font-size: 30px"}
[[We use the function [gg_resX]{style="color:indianred;"} of the [lindia]{style="color:indianred;"} package:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
#| fig.width: 6
#| fig.height: 3
gg_resX(mod_hills2, ncol = 2)
```
:::

[[üí° the function [gg_diagnose]{style="color:indianred;"} does all the diagnostic plots.]{style="font-size: 30px"}]{.fragment}

::: notes
Here the diagnostic is reasonably ok, except for the effect of the influential observations
:::


## Collinearity

[For multiple linear regression we need to check the absence of collinearity of the explanatory variables.]{style="font-size: 30px"}

[**If an explanatory variable is a linear combination of the other, this impedes the model fitting.**]{style="font-size: 30px"}

::::: columns
::: {.column width="70%"}

::: fragment 
[We can look at the correlation between variables:]{style="font-size: 30px"}

```{r}
cor(hills$dist_sd, hills$climb_sd)
```
:::

::: fragment 
[We also check that the VIF (*variance inflation factor*):]{style="font-size: 30px"}

```{r}
vif(mod_hills2)
```
:::

:::

::: {.column width="30%"}

[[‚ö†Ô∏è If the VIF of some explanatory variables is above 10 (or even 5), some explanatory variables should be excluded from the regression.]{style="font-size: 30px"}]{.fragment}

:::
:::::

::: notes
if the explanatory variables vary together, difficult to estimate their individual effects...

here we test the correlation between the standardised variables, but this would be the same for the non standardised...

there is some correlation

no strict rule for the VIF, this is context dependant. Check from 5
:::


## Presenting the results

[To represent graphically the effect of an explanatory variable, we can set the values of the other explanatory variables to a constant (for example their mean).]{style="font-size: 30px"}
[[Here we represent the effect of *dist_sd* setting *climb_sd* at the mean, -1 sd, + 1 sd:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
# create new values for the prediction
hills_new_dist <- expand.grid( # create a table with all the combinaison of 
  dist_sd = seq(-2, 2, 0.2), # new values for dist_sd
  climb_sd = c(-1, 0, 1)) # -1 sd, mean and +1 sd of climb_sd

# predict for new values
hills_pred_dist <- predict(mod_hills2, newdata = hills_new_dist, interval = "confidence")
hills_pred_dist <- cbind(hills_new_dist, hills_pred_dist)

# make the plot
p_mod2_dist <- ggplot(hills_pred_dist, aes(x = dist_sd, y = fit, 
                            color = as.factor(climb_sd), 
                            fill = as.factor(climb_sd))) +
    geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3) +
    geom_line()
```
:::

::: notes
as the variables are standardise, we can take -1, 0, 1

We can obviously do the same for climb_sd
:::

## Presenting the results

```{r}
p_mod2_dist
```

::: notes
the line are parallel
:::

## Presenting the results

::::: columns
::: {.column width="50%"}
[If we have many explanatory variables, we can plot all the coefficients with their confidence interval, using the function 
[ggcoef_model]{style="color:indianred;"} of the package [ggstats]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
ggcoef_model(mod_hills2)
```

:::

::: {.column width="50%"}

::: fragment
[or present a table of the coefficients, using [tbl_regression]{style="color:indianred;"} from the package [gtsummary]{style="color:indianred;"}]{style="font-size: 30px"}

```{r}
tbl_regression(mod_hills2, 
               intercept = TRUE)
```
:::
:::
:::::


## Interaction {.smaller}

[We can add an **interaction** term in the model to test if the effect of an explanatory variable depend of the value taken by another explanatory variable:]{style="font-size: 30px"}

$$y = \alpha + \beta_1 x_1 + \beta_2  x_2 + \beta_{12}  x_1 x_2 + \epsilon$$

[[which is equivalent to]{style="font-size: 30px"}]{.fragment}

::: fragment

$$y = \alpha + (\beta_1+\beta_{12} x_2) x_1 + \beta_2  x_2+ \epsilon$$
:::

[[and]{style="font-size: 30px"}]{.fragment}

::: fragment

$$y = \alpha + \beta_1  x_1+ (\beta_2+\beta_{12} x_1) x_2 +  \epsilon$$
:::

[[$\beta_{12}$ is the coefficient of the interaction. It represent the effect of $x_2$ on the relationship between y and $x_1$, and the effect of $x_1$ on the relationship between y and $x_2$.]{style="font-size: 30px"}]{.fragment}

::: notes
here we carry on with our model with two numerical variables, but could be for 1 cat and one numerical

as alaways epsilon is N(0, sigma2)
::: 

## Interaction {.smaller}

[To specify an interaction, we use [*]{style="color:indianred;"} in the formula:]{style="font-size: 30px"}

```{r}
mod_hills_inter <- lm(time ~ dist_sd * climb_sd, hills)
summary(mod_hills_inter)
```

## Visualisation of a model with interaction

[We can make a graph of the predictions as we did for the model without interaction:]{style="font-size: 30px"}

```{r}
# create new values for the prediction
hills_inter_new_dist <- expand.grid( # create a table with all the combinaison of 
  dist_sd = seq(-2, 2, 0.2), # new values for dist_sd
  climb_sd = c(-1, 0, 1)) # -1 sd, mean and +1 sd of climb_sd

# predict for new values
hills_inter_pred_dist <- predict(mod_hills_inter, 
                                 newdata = hills_inter_new_dist, 
                                 interval = "confidence")
hills_inter_new_dist <- cbind(hills_inter_new_dist, hills_inter_pred_dist)

# make the plot
p_mod_inter_dist <- ggplot(hills_inter_new_dist, aes(x = dist_sd, y = fit, 
                            color = as.factor(climb_sd), 
                            fill = as.factor(climb_sd))) +
    geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3) +
    geom_line()
```

## Visualisation of a model with interaction

```{r}
p_mod_inter_dist
```

[[The prediction lines are no longer parallel, because of the interaction term.]{style="font-size: 30px"}]{.fragment}

## Model selection

<!-- TO DO with my new model with interaction, comparing it with the model without interaction-->


Eric 57: AIC

et broom https://larmarange.github.io/guide-R/analyses/selection-modele-pas-a-pas.html#afficher-les-indicateurs-de-performance


<!-- fin de la partie sur les mod√®les lin√©aire multiples brouillon ci-dessous-->

<!-- think about moving the part on correlation if I decide to move it-->


# TO GO FURTHER (not in depth here...)

# What if the observation are not independant ? mixed-effect models

Nouragues per plot?


# What if the data are not normal ? GLM

## Logistic regression

karnataka pres/abs of a family per plot? mais pas de pr√©dicteur?
or BCI?

## Poisson

karnataka count of a family per plot? mais pas de pr√©dicteur?
or BCI?

Ressource : https://www.dataquest.io/blog/tutorial-poisson-regression-in-r/






# Acknowledgments {.smaller}

::: {.nonincremental}

* Marchand P. *Analyses et mod√©lisation des donn√©es √©cologiques* [in French](https://pmarchand1.github.io/ECL7102/){preview-link="false"}

* Marcon E. *cours-R-Geeft* [in French](https://ericmarcon.github.io/Cours-R-Geeft/){preview-link="false"}

* Guide R Lamarange

<!--
* *Introduction √† l‚Äôanalyse d‚Äôenqu√™tes avec R et RStudio* - Julien Barnier, Julien Biaudet, Fran√ßois Briatte, Milan Bouchet-Valat, Ewen Gallic, Fr√©d√©rique Giraud, Jo√´l Gombin, Mayeul Kauffmann, Christophe Lalanne, Joseph Larmarange, Nicolas Robette [in French](https://larmarange.github.io/analyse-R/){preview-link="false"} 

* Barnier J. *Introduction √† R et au tidyverse* [in French](https://juba.github.io/tidyverse/){preview-link="false"}

* Rech R. *One-Way ANOVA and Boxplot in R* [here](https://statdoe.com/one-way-anova-and-box-plot-in-r/){preview-link="false"}
-->

:::

# Ressources

::: {.nonincremental}

<!--

* Dytham C. *Choosing and using statistics - a biologist's guide*, 3rd Edition. Available [here](http://ngc.digitallibrary.co.in/bitstream/123456789/2272/1/Choosing%20and%20using%20statistics_A%20Biologist%27s%20Guide%20%283rd%20edition%29%40Dytham.pdf){preview-link="false"}

* Rech R. *One-Way ANOVA and Boxplot in R* [here](https://statdoe.com/one-way-anova-and-box-plot-in-r/){preview-link="false"}

* datacamp tutorials on [chi-square test](https://www.datacamp.com/tutorial/chi-square-test-r){preview-link="false"} and [t-test](https://www.datacamp.com/tutorial/t-tests-r-tutorial){preview-link="false"}, among other...

-->

:::

# Ressources to go further

* broom????





#########


## Log-normal distribution {.smaller}

$x \sim log\mathcal{N}(\mu, \sigma^2)$ if $log(x) \sim \mathcal{N}(\mu, \sigma^2)$

[We use the function [rnorm]{style="color:indianred;"} to randomly draw data from a normal distribution:]{style="font-size: 30px"}

::::: columns
::: {.column width="60%"}


```{r}
#| warning: false
dt1 <- data.frame(x = rlnorm(n = 100000, mean = 0, sd = 1))
dt2 <- data.frame(x = rlnorm(n = 100000, mean = 1, sd = 1))

ggplot() + 
  geom_density(data = dt1, aes(x = x), 
               color = "blue") + 
  geom_density(data = dt2, aes(x = x), 
               color = "green") + 
  scale_x_continuous(limits =c(0,20)) +
  theme_minimal()
```
:::

::: {.column width="40%"}

* The values are stricty positive.

* The variance increases when the mean increases. 

:::
:::::



# GLM?

binomial glm: cf p 254 book avec les pinguins



regression lin√©aire (en mnetionnant les GLM et les mixed-effect models)

include DHARma for model diagnostics?

Voir ici pour comparaison broom, modelr and DHARMa
https://chatgpt.com/share/680a764b-476c-8004-9a40-6c99c92a6d35



https://ericmarcon.github.io/Cours-R-Geeft/Regression_lineaire.html#2

