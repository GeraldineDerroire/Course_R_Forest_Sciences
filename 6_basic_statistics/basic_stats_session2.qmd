---
title: "Statistics with R - session 2"
author: Géraldine Derroire
institute: Cirad - UnB
date: last-modified
format: 
  revealjs:
    theme: solarized
    output-location: fragment 
    slide-number: true
    preview-links: true
    chalkboard: true
    link-external-icon: true
    link-external-newwindow: true
    incremental: true
execute:
  echo: true   
  warning: true
  message: true 
  cache: true
editor: 
  markdown: 
    wrap: sentence
---

#

Let's load the packages that we will use today

```{r}
#| message: false
library(tidyverse)
library(GGally)
```

::: notes
GGally to see pair-correlation
:::

#

Last session, we saw how to test for differences between variables.

Today we are going to look at **relationships between variables**.


# Covariance and correlation

## Covariance {.scrollable}

We have seen that the variance of a variable *x* is the mean of squared deviations from the mean:

$$\sigma^2_x = \frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n}$$

::: fragment

[The covariance between two quantitative variables *x* and *y* is the mean of the product of the deviations of *x* and *y* from their respective mean:]{style="font-size: 30px"}

$$cov_{xy} = \frac{\sum_{i=1}^n (x_i-\bar{x}) \times (y_i-\bar{y})}{n}$$

:::

## Covariance

We use the function [cov]{style="color:indianred;"} to get the covariance between two variables.

Let's test this with the iris data set, which you graphically explore in a last exercise. We focus on the species *versicolor*:

```{r}
data(iris)
iris <- as_tibble(iris)
iris_vers <- iris %>% filter(Species == "versicolor")
cov(iris_vers$Petal.Length, iris_vers$Petal.Width)
```
[[⚠️ The unit of the covariance is the product of the unit of x and of y...]{style="font-size: 30px"}]{.fragment}


## Correlation of Pearson

[To help with the interpretation of the covariance, we standardise the covariance by the product of the standard deviations.]{style="font-size: 30px"}

[This is the **coefficient of correlation of Pearson**, which is between -1 and 1:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

* [a coefficient = -1 (respectively 1) means that the variables are perfectly negatively (respectively positively) correlated]{style="font-size: 30px"}

* [a coefficient = 0 shows an absence of correlation]{style="font-size: 30px"}
:::

::: {.column width="50%"}

[[We use the function [cor]{style="color:indianred;"} to get the correlation between two variables (the default method is Pearson correlation).]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
cor(iris_vers$Petal.Length, 
    iris_vers$Petal.Width)
```
:::

[[*There is a strong positive correlation.*]{style="font-size: 30px"}]{.fragment}

:::
:::::


::: notes
So here strong positive correlation
:::

## Correlation of Spearman

[Pearson correlation makes the hypothesis that the relationship is linear.]{style="font-size: 30px"}

[If this is not the case, we can use the *rank correlation of Spearman*, by specifying the method:]{style="font-size: 30px"}

```{r}
cor(iris_vers$Petal.Length, iris_vers$Petal.Width,
    method = "spearman")
```
::: notes
here, not much difference
:::

## Test of correlation

[To test if the correlation is significant, we do a test of correlation with the function [cor.test]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
cor.test(iris_vers$Petal.Length, iris_vers$Petal.Width,
         method = "pearson")
```

[*The correlation is significant.*]{style="font-size: 30px"}

## Visualise correlations 

[Pearson correlations between multiple pairs of variables can be visualised using the function [ggpairs]{style="color:indianred;"} of the package [GGally]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
#| out.width: 80%
ggpairs(iris_vers, columns = c("Sepal.Length", "Sepal.Width",
                               "Petal.Length", "Petal.Width"))
# this is equivalent to 
 # ggpairs(iris_vers, columns = 1:4)
```

::: notes
we get the correlation coef and its significance

more complicated if we want other type of correlation than Pearson
:::

<!-- CONTINUE HERE-->




# linear model with one var

growth and WD

do the whole model diagnostic and ploting the results

# transfor logatyhtmique

marchand 3 et transfo logarythmique => pour modèle linéaire

## Log-normal distribution {.smaller}

$x \sim log\mathcal{N}(\mu, \sigma^2)$ if $log(x) \sim \mathcal{N}(\mu, \sigma^2)$

[We use the function [rnorm]{style="color:indianred;"} to randomly draw data from a normal distribution:]{style="font-size: 30px"}

::::: columns
::: {.column width="60%"}


```{r}
#| warning: false
dt1 <- data.frame(x = rlnorm(n = 100000, mean = 0, sd = 1))
dt2 <- data.frame(x = rlnorm(n = 100000, mean = 1, sd = 1))

ggplot() + 
  geom_density(data = dt1, aes(x = x), 
               color = "blue") + 
  geom_density(data = dt2, aes(x = x), 
               color = "green") + 
  scale_x_continuous(limits =c(0,20)) +
  theme_minimal()
```
:::

::: {.column width="40%"}

* The values are stricty positive.

* The variance increases when the mean increases. 

:::
:::::


# linear model with more than one var

* check for collinearity (before and after, check the VIF)

* add some other trais

* do some models selections

# mixed effect 

which data? 

# GLM?

binomial glm: cf p 254 book avec les pinguins



regression linéaire (en mnetionnant les GLM et les mixed-effect models)

include DHARma for model diagnostics?

Voir ici pour comparaison broom, modelr and DHARMa
https://chatgpt.com/share/680a764b-476c-8004-9a40-6c99c92a6d35


https://juba.github.io/tidyverse/04-bivarie.html#tests-statistiques

https://ericmarcon.github.io/Cours-R-Geeft/Regression_lineaire.html#2

