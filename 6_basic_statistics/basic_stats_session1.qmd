---
title: "Statistics with R - session 1"
author: Géraldine Derroire
institute: Cirad - UnB
date: last-modified
format: 
  revealjs:
    theme: solarized
    output-location: fragment 
    slide-number: true
    preview-links: true
    chalkboard: true
    link-external-icon: true
    link-external-newwindow: true
    incremental: true
execute:
  echo: true   
  warning: true
  message: true 
  cache: true
editor: 
  markdown: 
    wrap: sentence
---

#

Let's load the packages that we will use today

```{r}
#| message: false
library(tidyverse)
library(questionr)
library(BIOMASS)
library(GGally)
```

::: notes
tidyverse for data manip and graph
questionr for usefull function for exploration and visualisation
BIOMASS for data
GGally to see pair-correlation
:::


# Univariate statistics

::: notes
one variable
::: 

## Reminder on the descriptive statistics

For a categorical variable:

* Occurrence (count or proportion) per categories

* **visualisation**: [barplot, Cleveland plot]{.fragment}


## Reminder on the descriptive statistics

For a quantitative (numerical) variable:

* **measure of centrality**: [mean, median]{.fragment}

* **measure of dispersion**: [max-min, variance, standard deviation, interquartile distance, coefficient of variation]{.fragment}

* **visualisation**: histogram, boxplot, density plot

::: notes
have them guess
:::


## Reminder on the descriptive statistics

The coefficient (CV) is a standardised measure of dispersion, that can be used to compared the dispersion of variables measured in different scale:

$$CV = \frac{standard\:deviation}{mean}$$


## Descriptive statistics *vs* parameters

[**Descriptive statistics** are calculated from the observations (the data). These observations are a sample of a theoretical population that we don't measure.]{style="font-size: 30px"}

> [*eg*: The trees that we can measure in a forest plot are a sample of the population of the trees in the considered forest. We can calculate the mean diameter.]{style="font-size: 30px"}

[A theoretical population can be described by its **parameters**, that are not measured.]{style="font-size: 30px"}

> [*eg*: The mean diameter of the population of tree in the considered forest is a parameter.]{style="font-size: 30px"}


## Statistical distributions

We can use **statistical distributions** to represent the distribution of the values of the variable in the theoretical population. 

A statistical distribution (also called *probability law*) is a function that associates a probability with each possible value of a random variable.

## Statistical distributions

We can visualise a distribution density function (also called density of probability) with a density plot:

::::: columns
::: {.column width="70%"}
```{r}
#| echo: false
set.seed(22)
v <- data.frame(x = rnorm(n = 1000000))
p <- ggplot(v, aes(x = x)) + geom_density(color = "blue") + theme_minimal()
# Extract the computed density data
  density_data <- ggplot_build(p)$data[[1]]
# Filter density data 
  shade_data <- density_data %>% filter(x >= -1, x <= 1)
# Add on the plot
  p + geom_area(data = shade_data, aes(x = x, y = y), fill = "lightblue", alpha = 0.5) +
  geom_vline(xintercept = c(-1, 1), linetype = "dashed") + 
    scale_x_continuous(breaks = seq(from = -5, to = 5, by = 1)) + 
    labs(x = "x", y = "density")
```
:::

::: {.column width="30%"}
[[Here, the probability that $x \in [-1, 1]$ is equal to the blue area, which is the integral of the density function between -1 and 1.]{style="font-size: 30px"}]{.fragment}
:::
:::::

::: notes
The density function does not give the probability of being equal to a value, as for a continuous variable, this probability would be null.

:::

<!-- for each law => give examples of how used, show the equation with parameters, show the density (with several values of the param) -->

## Uniform distribution

$$\mathcal{U}(a, b)$$

[$[-a, b]$ is the range of possible values]{style="font-size: 30px"}

::::: columns
::: {.column width="30%"}
[*e.g.* for $\mathcal{U}(2, 5)$]{style="font-size: 30px"}
:::

::: {.column width="70%"}
```{r}
#| echo: false
dt <- data.frame(x = c(seq(from = -2, to = 7, by =1), 2, 5),
                 y = c(0, 0, 0 , 0, 0, 1, 1, 1, 0, 0, 1, 0))
dt0 <- filter(dt, x <= 2 & y==0)
dt00 <- filter(dt, x>=5 & y==0)
dt1 <- filter(dt, (x>=2 | x<=5) & y==1)
ggplot() + geom_line(data = dt, aes(x=x, y=y), colour = "blue", linetype="dashed") + 
  geom_line(data = dt0, aes(x=x, y=y), color = "blue", linewidth=1) + 
  geom_line(data = dt00, aes(x=x, y=y), color = "blue", linewidth=1) +
  geom_line(data = dt1, aes(x=x, y=y), color = "blue", linewidth=1) + theme_minimal() +
  scale_x_continuous(breaks = seq(from = -2, to = 7, by = 1)) + 
    labs(x = "x", y = "density")
```
:::
:::::
 

## Uniform distribution

We use the function [runif]{style="color:indianred;"} to randomly draw data from a uniform distribution:

```{r}
runif(n = 10, # number of value t
      min = 2, # lower limit of the distribution
      max = 5) # upper limit of the distribution
```
[*NB*: as we do a random draw, we get different values each time we run the command:]{.fragment}

::: fragment
```{r}
runif(n = 10, min = 2, max = 5) 
```
:::

## Law of large numbers

::::: columns
::: {.column width="50%"}
```{r}
hist(runif(n = 50,
           min = 2, max = 5))
```
:::

::: {.column width="50%"}
```{r}
hist(runif(n = 100000,
           min = 2, max = 5))
```
:::
:::::

[**The larger the size of the random sample, the closer the distribution of observed values is to the distribution of the theoretical population.**]{.fragment}



## Normal distribution

$$\mathcal{N}(\mu, \sigma^2)$$

* $\mu$ is the mean

* $\sigma^2 > 0$ is the variance, so $\sigma$ is the standard deviation

[💡 The normal distribution is also called *Gaussian distribution*.]{.fragment}

## Normal distribution {.smaller}

We use the function [rnorm]{style="color:indianred;"} to randomly draw data from a normal distribution:

::::: columns
::: {.column width="50%"}
```{r}
dt1 <- data.frame(x = rnorm(n = 100000, 
                            mean = 0, 
                            sd = 1))
dt2 <- data.frame(x = rnorm(n = 100000, 
                            mean = 4, 
                            sd = 0.5))
dt3 <- data.frame(x = rnorm(n = 100000, 
                            mean = -2, 
                            sd = 4))
g <- ggplot() + 
  geom_density(data = dt1, aes(x = x), 
               color = "blue") + 
  geom_density(data = dt2, aes(x = x), 
               color = "green") + 
  geom_density(data = dt3, aes(x = x), 
               color = "red") +
  theme_minimal()
```
:::

::: {.column width="50%"}
```{r}
g
```
:::
:::::

## Centred reduced normal distribution

If $x \sim \mathcal{N}(\mu, \sigma^2)$

then $z = \frac{x - \mu}{\sigma}$ follows a **centred reduced normal distribution**:

$z \sim \mathcal{N}(0, 1)$

[[💡 When to variable are measured in different scales, it can be usefull to standardise them by centring and reducing them, to be able to compare them.]{style="font-size: 30px"}]{.fragment}


## QQ-plot {.smaller}

[A *quantile-quantile* plot is used to visualise the correspondence between two distributions by plotting each percentile of the two distributions.]{style="font-size: 30px"}

[This is very useful to visually assess if a variable is normally-distributed, by comparing its distribution to a normal distribution.]{style="font-size: 30px"}

[We use the function [qqnorm]{style="color:indianred;"} to draw a normal QQ-plot:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 3
v1 <- rnorm(99, mean = 6, sd = 4)
qqnorm(v1)
qqline(v1)
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 4
#| fig-height: 3
v2 <- rnorm(100000, mean = 6, sd = 4)
qqnorm(v2)
qqline(v2)
```
:::
:::::

::: notes
if we see that they both follow a normal distribution, but the match is better when we increase the sampling size
:::


# Testing the association between two categorical variables

## Let's work with the the Nouragues tree data

[*Heights and diameters of trees in two 1-ha plots from the Nouragues forest (French Guiana)*]{style="font-size: 30px"}

[Let's load them:]{style="font-size: 30px"}

```{r, message=FALSE}
data(NouraguesHD) # load the data
dt_HD <- as_tibble(NouraguesHD) # rename and transform to tibble
rm(NouraguesHD) # remove
```

## Do the number of occurences of several genus depends on the plot? {.smaller}

[We are going to work on the 10 most abundant genus in the Nouragues dataset, let's select them:]{style="font-size: 30px"}


::::: columns
::: {.column width="50%"}
```{r}
# look at the 10 most abundant genus
abd_10 <- dt_HD %>% 
  filter(genus != "indet") %>%
  count(genus) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 10) 

abd_10
```
:::


::: {.column width="50%"}
::: fragment
```{r}
# take a subset of dt_HD containing the 10 most abundant genus
dt_sub10 <- dt_HD %>% 
  filter(genus %in% abd_10$genus) %>% 
  mutate(genus = as.factor(genus))
```
:::
:::
:::::


## Contingency table

[Let's start by doing a contingency table, using the function [table]{style="color:indianred;"}.]{style="font-size: 30px"}

[This gives the number of observations for each pairs of categories.]{style="font-size: 30px"}

```{r}
tab_abd <- table(dt_sub10$genus, dt_sub10$plotId)
tab_abd
```

## Percentages per category

[We can look at the percentages per column (*here* per plot), using the function [cprop]{style="color:indianred;"} of the package [questionr]{style="color:indianred;"}:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}
```{r}
cprop(tab_abd)
```
:::

::: {.column width="50%"}
[[⚠️ Here we look at the frequency per column because we are interested in the effect of the plot and the plot is in column. The function [lprop]{style="color:indianred;"} also exists to look at frequency per row.]{style="font-size: 30px"}]{.fragment}
:::
:::::

::: notes
We see that the occurence of most genus seems to depend on the plot
:::

::: notes
here in columns because...
:::

## Graphical exploration

[We can do a mosaic plot, using the function [mosaicplot]{style="color:indianred;"} to explore the association graphically:]{style="font-size: 30px"}

```{r}
mosaicplot(tab_abd)
```

## $\chi^2$ test {.smaller}

[We can then perform a $\chi^2$ test, to test the independence of the two categorical variables.
**H0: the variables are independent**]{style="font-size: 30px"}

::::: columns
::: {.column width="40%"}
[[We use the function [chisq.test]{style="color:indianred;"} on the **contingency table** (not on the percentage):]{style="font-size: 30px"}]{.fragment}
:::

::: {.column width="60%"}
::: fragment
```{r}
res_chi2 <- chisq.test(tab_abd)
res_chi2
```
:::
:::
:::::

[[The result shows the following values:]{style="font-size: 30px"}]{.fragment}

* [*X-squared* is the value of the $\chi^2$ statistics, which is a "distance" between the observed occurrences and the expected ones if the variables were independent.]{style="font-size: 28px"}

* [*df* is the number of degrees of freedom]{style="font-size: 28px"}

* [*p-value* is the probability to get such a *X-squared* value under the assumption of independence.]{style="font-size: 28px"}
[[**Here the *p-value* is very low, so we can reject the null hypothesis of independence.**]{style="font-size: 28px"}]{.fragment}


## $\chi^2$ test {.smaller}

* [The $\chi^2$ needs to be done on contingency tables and not on proportions, as the size of the sample needs to be known.]{style="font-size: 30px"}

* [It is less reliable if the sample size is too small. It should not be used if any of the expected frequency under H0 is below 5. Let's check this:]{style="font-size: 30px"}

::: fragment
```{r}
res_chi2$expected
```
:::

::: notes
Here it is ok
:::

## Residuals 

::::: columns
::: {.column width="40%"}

[We can look at the residuals using the function [chisq.residuals]{style="color:indianred;"} of the package [questionr]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
chisq.residuals(tab_abd)
```
:::

::: {.column width="60%}

* [residuals < -2 indicate **under-representation**]{style="font-size: 28px"}

* [residuals > 2 indicate **over-representation**]{style="font-size: 28px"}

* [residuals in [-2, 2] indicate **no significant difference** to independence]{style="font-size: 28px"}

[[*The threshold of 2 correspond to a 95% confidence interval.*]{style="font-size: 30px"}]{.fragment}
:::

::::

## Residuals

[We can also present the residuals graphically with the argument *shade = TRUE* in the function [mosaicplot]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
#| out.width: 80%
mosaicplot(tab_abd, 
           shade = TRUE,
           las = 3, # vertical orientation of labels
           main = "") # remove main title
```


# Testing if two groups differ considering a quantitative variable

## Let's work with the Nouragues tree data

[We will focus on two genus: *Dicorynia* and *Protium*:]{style="font-size: 30px"}

```{r, message=FALSE, eval = FALSE, echo=FALSE}
# for me to choose the most abundant sp with a distrib that works well
abund <- dt_HD %>% count(genus) %>% arrange(desc(n)) %>% slice_head(n = 10) 

dt_HD %>% filter(genus %in% abund$genus) %>% 
  ggplot(aes(x=genus, y=H)) + geom_boxplot()
```

```{r}
# one dataset with both genus
dt_sub <- dt_HD %>% 
  filter(genus %in% c("Dicorynia", "Protium"))

# one dataset per genus
d_Dico <- filter(dt_sub, genus == "Dicorynia")
d_Prot <- filter(dt_sub, genus == "Protium")
```

::: notes
We do both a common dataset and two dataset per genus
:::

## Do the two genus differ in height?

[Let's explore this graphically:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out.width: 80%
dt_sub %>% 
  ggplot(aes(x=genus, y=H)) + 
  geom_boxplot()
```


## Are the distribution of height in both genus normal?

[To decide which test to use, we first check the normality of the distribution with a Shapiro-Wilk test, using the function [shapiro.test]{style="color:indianred;"}]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}
```{r}
shapiro.test(d_Dico$H)
```
:::

::: {.column width="50%"}
```{r}
shapiro.test(d_Prot$H)
```
:::
:::::

[[The tests are non-significant, so they don't reject the hypothesis of normality. 
We can consider the two distributions as normal.]{style="font-size: 30px"}]{.fragment}

## Are the distribution of height in both genus normal?

[We could also check the normality graphically with normal Q-Q plot:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 2.8
#| fig-height: 2.8
qqnorm(d_Dico$H)
qqline(d_Dico$H)
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 2.8
#| fig-height: 2.8
qqnorm(d_Prot$H)
qqline(d_Prot$H)
```
:::
:::::

[The points fall reasonably on the diagonal line, we can consider the data as normally distributed.]{style="font-size: 25px"}]{.fragment}


## t-test {.smaller}

[As the two distributions are normal, we can do a t-test using the function [t.test]{style="color:indianred;"}:]{style="font-size: 30px"}

[[A t-test test the equality of means of two groups: **H0: equality of the means**]{style="font-size: 30px"}]{.fragment}

::::: columns
::: {.column width="65%"}

::: fragment
```{r}
t.test(dt_sub$H ~ dt_sub$genus)
```
:::
:::

::: {.column width="35%"}
[[The result is significant, so we can reject the null hypothesis that the mean of both group are equal and conclude that the two genus differ in height.]{style="font-size: 30px"}]{.fragment}
:::
:::::

[[💡 The result indicates that we performed a *Welch t-test*, which doesn't assume homoscedasticity (equality of variance). If we want to run a t-test assuming homoscedasticity, we need to use the argument *var.equal = TRUE*. ]{style="font-size: 25px"}]{.fragment}

::: notes
the Welch modification is done because it handle better unequal variances and unequal sample size

the test tell us that the difference between the two is between 6.67 and 17.18 meters (95% confidence)
::: 

## Do the two genus differ in diameter?

[Let's explore this graphically:]{style="font-size: 30px"}

```{r}
#| warning: false
#| out.width: 80%
dt_sub %>% 
  ggplot(aes(x=genus, y=D)) + 
  geom_boxplot()
```

## Are the distribution of diameter in both genus normal?

[Let's first test if the distribution of diameters are normal:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}
```{r}
shapiro.test(d_Dico$D)
```
:::

::: {.column width="50%"}
```{r}
shapiro.test(d_Prot$D)
```
:::
:::::

[[The tests are significant, so they reject the hypothesis of normality.]{style="font-size: 30px"}]{.fragment}

## Are the distribution of diameter in both genus normal?

[This is how the QQ plots look:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 2.8
#| fig-height: 2.8
qqnorm(d_Dico$D)
qqline(d_Dico$D)
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 2.8
#| fig-height: 2.8
qqnorm(d_Prot$D)
qqline(d_Prot$D)
```
:::
:::::

[[The distributions of diameters are not normal, so we cannot use a t-test.]{style="font-size: 30px"}]{.fragment}

## Wikcoxon rank test

[We can do a Wilcoxon rank test, which is the non-parametric equivalent to the t-test, using the function [wilcox.test]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
wilcox.test(dt_sub$H ~dt_sub$genus)
```

[[The result is significant, so we can reject the null hypothesis, and conclude that the two genus differ in diameter.]{style="font-size: 30px"}]{.fragment}

[💡 A non-parametric test is a test that does not make any assumption regarding the distribution of the observations.]


::: notes
The test assume all values to be different, but there are some ties (equal values) so the p-value is approximates
=> here very small so OK
:::


## What if the data are paired?

[Let's load some data:]{style="font-size: 30px"} (available [here](https://geraldinederroire.github.io/Course_R_Forest_Sciences/6_basic_statistics/data/data_statistics_1.RData){preview-link="false"})

```{r}
load("data/data_statistics_1.RData")
```

[[The data *dt_thin* is a dummy dataset of the growth (in cm DBH/year) of 100 tree before and after thinning.]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
str(dt_thin)
```
:::

[[These data are **paired**, as each individual tree is measured twice: before and after thinning.]{style="font-size: 30px"}]{.fragment}


## Growth before and after thinning

[Let's explore the data graphically:]{style="font-size: 30px"}

```{r}
#| out.width: 80%
ggplot(dt_thin, aes(x=treat, y=growth)) + geom_boxplot()
```

## Growth before and after thinning

[First, we pivot the data to a wide format:]{style="font-size: 30px"}

```{r}
dt_thin_w <- dt_thin %>% 
  pivot_wider(names_from = "treat", 
              values_from = "growth")

slice_head(dt_thin_w, n = 5) # check the first rows
```

## Are the distributions normal?

[Let's check the normality graphically:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 3
#| fig-height: 3
qqnorm(dt_thin_w$before)
qqline(dt_thin_w$before)
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 3
#| fig-height: 3
qqnorm(dt_thin_w$after)
qqline(dt_thin_w$after)
```
:::
:::::

::: notes
no needs to check with a test because

* qq plot are good and visual explo OK

* t-test is relatively robust to violation of normality assumption
:::

## t-test for paired data

[We can do a t-test for paired data by specifying **paired = TRUE** in the function [t.test]{style="color:indianred;"}:]{style="font-size: 30px"}

::::: columns
::: {.column width="60%"}
```{r}
res_paired <- t.test(dt_thin_w$before, 
                     dt_thin_w$after,
                     paired=T)
res_paired
```
:::

::: {.column width="40%"}
[[The result is significant, so we can conclude that the growths before and after thinning differ.]{style="font-size: 30px"}]{.fragment}

[[💡 If the data are not normally distributed, we can do a Wikcoxon rank test for paired data in a similar way]{style="font-size: 30px"}]{.fragment}
:::
:::::

::: notes
Note that the synthax has changed

give the 95% confidence interval of the difference (before - after)
::: 

## What to report for a t-test? {.smaller}

[To retrieve the p-value:]{style="font-size: 30px"}

```{r}
res_paired$p.value
```

[[A significant p-value indicates that it is highly improbable to have the observed effect if the null hypothesis is true. 
So we can conclude that the two groups differ.
**⚠️ However, this does not inform on the magnitude of the difference.**]{style="font-size: 30px"}]{.fragment}

[[In addition to the p-value, we need to report:]{style="font-size: 30px"}]{.fragment}

::::: columns
::: {.column width="50%"}
* [the magnitude of the difference]{style="font-size: 30px"}

::: fragment
```{r}
res_paired$estimate
```
:::
:::

::: {.column width="50%"}

* [the confidence interval of the difference]{style="font-size: 30px"}

::: fragment
```{r}
res_paired$conf.int
```
:::
:::
:::::

::: notes
the attr 0.95 is the confidence level (95% by default, but this can be changed
:::



# Testing difference between more than two groups


## Analysis of variance: ANOVA

With the t-test, we tested the difference between two groups. 

If we have more than two groups, we use the **ANOVA**.

The ANOVA compares the variation within groups with the variation between groups.

**H0: all the observations come from populations with a same mean.**



## Do the height of several genus differ? {.scrollable}

[We extent the analysis done on *Dicorynia* and *Protium* to more genus: 
let's consider the 10 more abundant genus.]{style="font-size: 30px"}

[[Let's start with a graphical exploration:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
#| warning: false
#| out.width: 70%
ggplot(dt_sub10, aes(x=genus, y=H)) + 
  geom_boxplot()
```
:::

## ANOVA

[We perform the ANOVA using the function [aov]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
H_aov <- aov(H ~ genus,
             data = dt_sub10)
summary(H_aov)
```
[[**The test is significant, meaning that the groups differ.**]{style="font-size: 30px"}]{.fragment}


## Checking the model assumptions

[The ANOVA assumes **homoscedasticity**, meaning a constant variance variance of residuals across groups.]{style="font-size: 30px"}

[We check this graphically by plotting the residuals against the fitted values (the estimated mean for each group):]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

```{r}
#| fig.width: 5
#| fig.height: 3.5
plot(H_aov, which = 1)
```
:::

::: {.column width="50%"}

::: fragment
[There is no strong pattern.]{style="font-size: 30px"}

[The residuals are distributed around 0 in a similar way for each groups.]{style="font-size: 30px"}

[**We can consider that the assumptions of homoscedasticity is respected.**]{style="font-size: 30px"}
:::
:::
:::::

## Checking the model assumptions

[The ANOVA assumes that the **residuals are normally distributed**.]{style="font-size: 30px"}

[We check this graphically with a Q-Q plot:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

```{r}
#| fig.width: 5
#| fig.height: 3.5
plot(H_aov, which = 2)
```
:::

::: {.column width="50%"}

::: fragment
[The points are mostly on the diagonal.]{style="font-size: 30px"}

[**We can consider that the assumptions of normality of the distribution of the residuals is respected.**]{style="font-size: 30px"}
:::
:::
:::::


::: notes
we don't check that the values are normally distributed, but the residuals
:::



## Estimated mean per group

[The get the estimated mean of each group, we use the function [coef]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
coef(H_aov)
```
[[The *intercept* correspond to the first group (here *Dicorynia*).]{style="font-size: 30px"}]{.fragment}

[[The other coefficient show the difference of each group with the first group. For instance, the mean of *Sterculia* is:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
coef(H_aov)[1] + coef(H_aov)[10]
```
:::


## Comparing the groups 

[We know that the means of the groups differ, but we also would like to know which groups significantly differ from the others.]{style="font-size: 30px"}

[We can perform a Tukey test, which compares all pairs of groups, using the function [TukeyHSD]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
H_tukey <- TukeyHSD(H_aov)
H_tukey
```

::: notes
We get the comparison of all pairs
:::

## Graphical presentation of the results 

[We use the function [multcompLetters4]{style="color:indianred;"} from the package [multcompView]{style="color:indianred;"} to get the letters indicating identical groups:]{style="font-size: 30px"}

```{r}
library(multcompView)
H_letters <- multcompLetters4(H_aov, H_tukey)
H_letters
```

[[This gives a list, from which we extract the letters:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
H_letters_tb <- tibble(
  genus = names(H_letters[[1]]$Letters),
  letter = as.character(H_letters[[1]]$Letters)
  )
```
:::

::: notes
to understand why we do that str(H_letters)
:::


## Graphical presentation of the results 

[We then make a tibble containing the groups, the letters and the 75th percentile (to position the letter above the box):]{style="font-size: 30px"}

```{r}
#| warning: false
H_letters_pos <- dt_sub10 %>% 
  group_by(genus) %>% 
  summarise(q_75 = quantile(H, probs = 0.75, na.rm = TRUE)) %>% 
  inner_join(H_letters_tb)
```

[[And we make the plot:]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
graph_H <- ggplot() +
  geom_boxplot(data = dt_sub10, 
               aes(x = genus, y = H, fill = genus),
               show.legend = FALSE) +
  geom_text(data = H_letters_pos,
            aes(x = genus, y = q_75, # position of the label
                label = letter), # labels are the letters
            size = 5, # size of the labels
            vjust = -1, hjust = -1) + # adjusts position of the labels
  scale_fill_brewer(palette = "Blues") + 
  theme_bw()
```
:::

## Graphical presentation of the results

```{r}
#| warning: false
#| out.width: 120%
graph_H

```

## What to do if the assumptions of the ANOVA are not met?

[If the assumption of normality of the residuals is not met, we can:]{style="font-size: 30px"}

* [Perform a **Kruskal-Wallis test** (non parametric) using the function [kruskal.test]{style="color:indianred;"}]{style="font-size: 30px"}

* [Transform the continuous variable (log, sqrt...)]{style="font-size: 30px"}

* [Perform a **generalised linear model** (GLM)]{style="font-size: 30px"}

[[If the assumption of homoscedasticity is not met, we can perform a **Welch's ANOVA** using the function [oneway.test]{style="color:indianred;"}]{style="font-size: 30px"}]{.fragment}


# 

So far, we saw how to test for differences between variables.

Now and iin the next session we are going to look at **relationships between variables**.


# Covariance and correlation

## Covariance {.scrollable}

We have seen that the variance of a variable *x* is the mean of squared deviations from the mean:

$$\sigma^2_x = \frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n}$$

::: fragment

[The covariance between two numerical variables *x* and *y* is the mean of the product of the deviations of *x* and *y* from their respective mean:]{style="font-size: 30px"}

$$cov_{xy} = \frac{\sum_{i=1}^n (x_i-\bar{x}) \times (y_i-\bar{y})}{n}$$

:::

## Covariance

We use the function [cov]{style="color:indianred;"} to get the covariance between two variables.

Let's test this with the iris data set, which you graphically explore in a last exercise. We focus on the species *versicolor*:

```{r}
data(iris)
iris <- as_tibble(iris)
iris_vers <- iris %>% filter(Species == "versicolor")
cov(iris_vers$Petal.Length, iris_vers$Petal.Width)
```
[[⚠️ The unit of the covariance is the product of the unit of x and of y...]{style="font-size: 30px"}]{.fragment}


## Correlation of Pearson

[To help with the interpretation of the covariance, we standardise the covariance by the product of the standard deviations.]{style="font-size: 30px"}

[This is the **coefficient of correlation of Pearson**, which is between -1 and 1:]{style="font-size: 30px"}

::::: columns
::: {.column width="50%"}

* [a coefficient = -1 (respectively 1) means that the variables are perfectly negatively (respectively positively) correlated]{style="font-size: 30px"}

* [a coefficient = 0 shows an absence of correlation]{style="font-size: 30px"}
:::

::: {.column width="50%"}

[[We use the function [cor]{style="color:indianred;"} to get the correlation between two variables (the default method is Pearson correlation).]{style="font-size: 30px"}]{.fragment}

::: fragment
```{r}
cor(iris_vers$Petal.Length, 
    iris_vers$Petal.Width)
```
:::

[[*There is a strong positive correlation.*]{style="font-size: 30px"}]{.fragment}

:::
:::::


::: notes
So here strong positive correlation
:::

## Correlation of Spearman

[Pearson correlation makes the hypothesis that the relationship is linear.]{style="font-size: 30px"}

[If this is not the case, we can use the *rank correlation of Spearman*, by specifying the method:]{style="font-size: 30px"}

```{r}
cor(iris_vers$Petal.Length, iris_vers$Petal.Width,
    method = "spearman")
```
::: notes
here, not much difference
:::

## Test of correlation

[To test if the correlation is significant, we do a test of correlation with the function [cor.test]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
cor.test(iris_vers$Petal.Length, iris_vers$Petal.Width,
         method = "pearson")
```

[[*The correlation is significant.*]{style="font-size: 30px"}]{.fragment}

## Visualise correlations 

[Pearson correlations between multiple pairs of variables can be visualised using the function [ggpairs]{style="color:indianred;"} of the package [GGally]{style="color:indianred;"}:]{style="font-size: 30px"}

```{r}
#| out.width: 80%
ggpairs(iris_vers, columns = c("Sepal.Length", "Sepal.Width",
                               "Petal.Length", "Petal.Width"))
# this is equivalent to 
 # ggpairs(iris_vers, columns = 1:4)
```

::: notes
we get the correlation coef and its significance

more complicated if we want other type of correlation than Pearson
:::




# Acknowledgments {.smaller}

::: {.nonincremental}

* Marchand P. *Analyses et modélisation des données écologiques* [in French](https://pmarchand1.github.io/ECL7102/){preview-link="false"}

* Marcon E. *cours-R-Geeft* [in French](https://ericmarcon.github.io/Cours-R-Geeft/){preview-link="false"}

* *Introduction à l’analyse d’enquêtes avec R et RStudio* - Julien Barnier, Julien Biaudet, François Briatte, Milan Bouchet-Valat, Ewen Gallic, Frédérique Giraud, Joël Gombin, Mayeul Kauffmann, Christophe Lalanne, Joseph Larmarange, Nicolas Robette [in French](https://larmarange.github.io/analyse-R/){preview-link="false"} 

* Barnier J. *Introduction à R et au tidyverse* [in French](https://juba.github.io/tidyverse/){preview-link="false"}

* Rech R. *One-Way ANOVA and Boxplot in R* [here](https://statdoe.com/one-way-anova-and-box-plot-in-r/){preview-link="false"}

:::

# Ressources

::: {.nonincremental}

* Dytham C. *Choosing and using statistics - a biologist's guide*, 3rd Edition. Available [here](http://ngc.digitallibrary.co.in/bitstream/123456789/2272/1/Choosing%20and%20using%20statistics_A%20Biologist%27s%20Guide%20%283rd%20edition%29%40Dytham.pdf){preview-link="false"}

* Rech R. *One-Way ANOVA and Boxplot in R* [here](https://statdoe.com/one-way-anova-and-box-plot-in-r/){preview-link="false"}

* datacamp tutorials on [chi-square test](https://www.datacamp.com/tutorial/chi-square-test-r){preview-link="false"} and [t-test](https://www.datacamp.com/tutorial/t-tests-r-tutorial){preview-link="false"}, among other...

:::


